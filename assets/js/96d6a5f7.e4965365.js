"use strict";(self.webpackChunkapache_streampark_website=self.webpackChunkapache_streampark_website||[]).push([[8915],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>m});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),c=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},p=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),d=c(n),m=r,h=d["".concat(l,".").concat(m)]||d[m]||u[m]||o;return n?a.createElement(h,i(i({ref:t},p),{},{components:n})):a.createElement(h,i({ref:t},p))}));function m(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var c=2;c<o;c++)i[c]=n[c];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},8949:(e,t,n)=>{n.d(t,{Z:()=>i});var a=n(7294),r=n(6010);const o="tabItem_Ymn6";function i(e){let{children:t,hidden:n,className:i}=e;return a.createElement("div",{role:"tabpanel",className:(0,r.Z)(o,i),hidden:n},t)}},5488:(e,t,n)=>{n.d(t,{Z:()=>m});var a=n(7462),r=n(7294),o=n(6010),i=n(2389),s=n(7392),l=n(7094),c=n(2466);const p="tabList__CuJ",u="tabItem_LNqP";function d(e){var t;const{lazy:n,block:i,defaultValue:d,values:m,groupId:h,className:f}=e,g=r.Children.map(e.children,(e=>{if((0,r.isValidElement)(e)&&"value"in e.props)return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})),y=m??g.map((e=>{let{props:{value:t,label:n,attributes:a}}=e;return{value:t,label:n,attributes:a}})),k=(0,s.l)(y,((e,t)=>e.value===t.value));if(k.length>0)throw new Error(`Docusaurus error: Duplicate values "${k.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`);const b=null===d?d:d??(null==(t=g.find((e=>e.props.default)))?void 0:t.props.value)??g[0].props.value;if(null!==b&&!y.some((e=>e.value===b)))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${b}" but none of its children has the corresponding value. Available values are: ${y.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);const{tabGroupChoices:v,setTabGroupChoices:S}=(0,l.U)(),[w,T]=(0,r.useState)(b),H=[],{blockElementScrollPositionUntilNextRender:O}=(0,c.o5)();if(null!=h){const e=v[h];null!=e&&e!==w&&y.some((t=>t.value===e))&&T(e)}const E=e=>{const t=e.currentTarget,n=H.indexOf(t),a=y[n].value;a!==w&&(O(t),T(a),null!=h&&S(h,String(a)))},D=e=>{var t;let n=null;switch(e.key){case"ArrowRight":{const t=H.indexOf(e.currentTarget)+1;n=H[t]??H[0];break}case"ArrowLeft":{const t=H.indexOf(e.currentTarget)-1;n=H[t]??H[H.length-1];break}}null==(t=n)||t.focus()};return r.createElement("div",{className:(0,o.Z)("tabs-container",p)},r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.Z)("tabs",{"tabs--block":i},f)},y.map((e=>{let{value:t,label:n,attributes:i}=e;return r.createElement("li",(0,a.Z)({role:"tab",tabIndex:w===t?0:-1,"aria-selected":w===t,key:t,ref:e=>H.push(e),onKeyDown:D,onFocus:E,onClick:E},i,{className:(0,o.Z)("tabs__item",u,null==i?void 0:i.className,{"tabs__item--active":w===t})}),n??t)}))),n?(0,r.cloneElement)(g.filter((e=>e.props.value===w))[0],{className:"margin-top--md"}):r.createElement("div",{className:"margin-top--md"},g.map(((e,t)=>(0,r.cloneElement)(e,{key:t,hidden:e.props.value!==w})))))}function m(e){const t=(0,i.Z)();return r.createElement(d,(0,a.Z)({key:String(t)},e))}},5655:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>l,default:()=>m,frontMatter:()=>s,metadata:()=>c,toc:()=>u});var a=n(7462),r=(n(7294),n(3905)),o=n(5488),i=n(8949);const s={id:"Http-Connector",title:"Http Connector",original:!0,sidebar_position:7},l=void 0,c={unversionedId:"connector/Http-Connector",id:"connector/Http-Connector",title:"Http Connector",description:"Some background services receive data through HTTP requests. In this scenario, Flink can write result data through HTTP",source:"@site/docs/connector/7-http.md",sourceDirName:"connector",slug:"/connector/Http-Connector",permalink:"/docs/connector/Http-Connector",draft:!1,editUrl:"https://github.com/apache/incubator-streampark-website/edit/dev/docs/connector/7-http.md",tags:[],version:"current",sidebarPosition:7,frontMatter:{id:"Http-Connector",title:"Http Connector",original:!0,sidebar_position:7},sidebar:"tutorialSidebar",previous:{title:"Apache HBase Connector",permalink:"/docs/connector/Hbase-Connector"},next:{title:"Redis Connector",permalink:"/docs/connector/Redis-Connector"}},p={},u=[{value:"http asynchronous write",id:"http-asynchronous-write",level:2},{value:"Write with StreamPark",id:"write-with-streampark",level:2},{value:"http asynchronous write support type",id:"http-asynchronous-write-support-type",level:3},{value:"Configuration list of HTTP asynchronous write",id:"configuration-list-of-http-asynchronous-write",level:3},{value:"HTTP writes data asynchronously",id:"http-writes-data-asynchronously",level:3},{value:"Other configuration",id:"other-configuration",level:2}],d={toc:u};function m(e){let{components:t,...n}=e;return(0,r.kt)("wrapper",(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"Some background services receive data through HTTP requests. In this scenario, Flink can write result data through HTTP\nrequests. Currently, Flink officially does not provide a connector for writing data through HTTP requests. StreamPark\nencapsulates HttpSink to write data asynchronously in real-time based on asynchttpclient."),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"HttpSink")," writes do not support transactions, writing data to the target service provides AT_LEAST_ONCE semantics. Data\nthat fails to be retried multiple times will be written to external components (kafka, mysql, hdfs, hbase), and the data\nwill be restored manually to achieve final data consistency."),(0,r.kt)("h2",{id:"http-asynchronous-write"},"http asynchronous write"),(0,r.kt)("p",null,"Asynchronous writing uses asynchttpclient as the client, you need to import the jar of asynchttpclient first."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"\n<dependency>\n    <groupId>org.asynchttpclient</groupId>\n    <artifactId>async-http-client</artifactId>\n    <optional>true</optional>\n</dependency>\n")),(0,r.kt)("h2",{id:"write-with-streampark"},"Write with StreamPark"),(0,r.kt)("h3",{id:"http-asynchronous-write-support-type"},"http asynchronous write support type"),(0,r.kt)("p",null,"HttpSink supports get , post , patch , put , delete , options , trace of http protocol. Corresponding to the method of\nthe same name of HttpSink, the specific information is as follows:"),(0,r.kt)(i.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},"class HttpSink(@(transient@param) ctx: StreamingContext,\n               header: Map[String, String] = Map.empty[String, String],\n               parallelism: Int = 0,\n               name: String = null,\n               uid: String = null) extends Sink {\n\n  def get(stream: DataStream[String]): DataStreamSink[String] = sink(stream, HttpGet.METHOD_NAME)\n\n  def post(stream: DataStream[String]): DataStreamSink[String] = sink(stream, HttpPost.METHOD_NAME)\n\n  def patch(stream: DataStream[String]): DataStreamSink[String] = sink(stream, HttpPatch.METHOD_NAME)\n\n  def put(stream: DataStream[String]): DataStreamSink[String] = sink(stream, HttpPut.METHOD_NAME)\n\n  def delete(stream: DataStream[String]): DataStreamSink[String] = sink(stream, HttpDelete.METHOD_NAME)\n\n  def options(stream: DataStream[String]): DataStreamSink[String] = sink(stream, HttpOptions.METHOD_NAME)\n\n  def trace(stream: DataStream[String]): DataStreamSink[String] = sink(stream, HttpTrace.METHOD_NAME)\n\n  private[this] def sink(stream: DataStream[String], method: String): DataStreamSink[String] = {\n    val params = ctx.parameter.toMap.filter(_._1.startsWith(HTTP_SINK_PREFIX)).map(x => x._1.drop(HTTP_SINK_PREFIX.length + 1) -> x._2)\n    val sinkFun = new HttpSinkFunction(params, header, method)\n    val sink = stream.addSink(sinkFun)\n    afterSink(sink, parallelism, name, uid)\n  }\n}\n\n"))),(0,r.kt)("h3",{id:"configuration-list-of-http-asynchronous-write"},"Configuration list of HTTP asynchronous write"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"http.sink:\n  threshold:\n    numWriters: 3\n    queueCapacity: 10000 #The maximum capacity of the queue, according to the size of a single record, and the size of the queue is estimated by itself. If the value is too large, the upstream data source is coming too fast, and the downstream write data may not keep up with OOM.\n    timeout: 100 #Timeout for sending http requests\n    retries: 3 #Maximum number of retries when sending fails\n    successCode: 200 #Send success status code\n  failover:\n    table: record\n    storage: mysql #kafka,hbase,hdfs\n    jdbc:\n      jdbcUrl: jdbc:mysql://localhost:3306/test\n      username: root\n      password: 123456\n    kafka:\n      topic: bigdata\n      bootstrap.servers: localhost:9091,localhost:9092,localhost:9093\n    hbase:\n      zookeeper.quorum: localhost\n      zookeeper.property.clientPort: 2181\n    hdfs:\n      namenode: hdfs://localhost:8020 # namenode rpc address and port, e.g: hdfs://hadoop:8020 , hdfs://hadoop:9000\n      user: benjobs # user\n      path: /http/failover # save path\n      format: yyyy-MM-dd\n")),(0,r.kt)("h3",{id:"http-writes-data-asynchronously"},"HTTP writes data asynchronously"),(0,r.kt)("p",null,"The program sample is scala"),(0,r.kt)(o.Z,{mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'\nimport org.apache.streampark.flink.core.scala.FlinkStreaming\nimport org.apache.streampark.flink.core.scala.sink.HttpSink\nimport org.apache.flink.api.scala._\nimport org.apache.flink.streaming.api.scala.DataStream\n\nobject HttpSinkApp extends FlinkStreaming {\n\n  override def handle(): Unit = {\n\n    val source = context.addSource(new TestSource)\n\n    val value: DataStream[String] = source.map(x => s"http://127.0.0.1:8080?userId=(${x.userId}&siteId=${x.siteId})")\n    HttpSink().post(value).setParallelism(1)\n\n  }\n}\n\n')))),(0,r.kt)("admonition",{title:"warn",type:"info"},(0,r.kt)("p",{parentName:"admonition"},"Since http can only write one piece of data at a time, the latency is relatively high, and it is not suitable for\nwriting large amounts of data.   It is necessary to set a reasonable threshold to improve performance.\nSince httpSink asynchronous writing fails, data will be added to the cache queue again, which may cause data in the same\nwindow to be written in two batches.   It is recommended to fully test in scenarios with high real-time requirements.\nAfter the asynchronous write data reaches the maximum retry value, the data will be backed up to the external component, and the component connection will be initialized at this time. It is recommended to ensure the availability of the failover component.")),(0,r.kt)("h2",{id:"other-configuration"},"Other configuration"),(0,r.kt)("p",null,"All other configurations must comply with the ",(0,r.kt)("strong",{parentName:"p"},"StreamPark")," configuration.\nFor specific configurable items and the role of each parameter, please refer ",(0,r.kt)("a",{parentName:"p",href:"/docs/development/conf"},"Project configuration")))}m.isMDXComponent=!0}}]);