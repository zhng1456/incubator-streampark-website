"use strict";(self.webpackChunkapache_streampark_website=self.webpackChunkapache_streampark_website||[]).push([[6510],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>m});var a=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=a.createContext({}),c=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},p=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),p=c(n),m=o,h=p["".concat(l,".").concat(m)]||p[m]||d[m]||r;return n?a.createElement(h,i(i({ref:t},u),{},{components:n})):a.createElement(h,i({ref:t},u))}));function m(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,i=new Array(r);i[0]=p;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:o,i[1]=s;for(var c=2;c<r;c++)i[c]=n[c];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}p.displayName="MDXCreateElement"},8949:(e,t,n)=>{n.d(t,{Z:()=>i});var a=n(7294),o=n(6010);const r="tabItem_Ymn6";function i(e){let{children:t,hidden:n,className:i}=e;return a.createElement("div",{role:"tabpanel",className:(0,o.Z)(r,i),hidden:n},t)}},5488:(e,t,n)=>{n.d(t,{Z:()=>m});var a=n(7462),o=n(7294),r=n(6010),i=n(2389),s=n(7392),l=n(7094),c=n(2466);const u="tabList__CuJ",d="tabItem_LNqP";function p(e){var t;const{lazy:n,block:i,defaultValue:p,values:m,groupId:h,className:f}=e,k=o.Children.map(e.children,(e=>{if((0,o.isValidElement)(e)&&"value"in e.props)return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})),b=m??k.map((e=>{let{props:{value:t,label:n,attributes:a}}=e;return{value:t,label:n,attributes:a}})),y=(0,s.l)(b,((e,t)=>e.value===t.value));if(y.length>0)throw new Error(`Docusaurus error: Duplicate values "${y.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`);const g=null===p?p:p??(null==(t=k.find((e=>e.props.default)))?void 0:t.props.value)??k[0].props.value;if(null!==g&&!b.some((e=>e.value===g)))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${g}" but none of its children has the corresponding value. Available values are: ${b.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);const{tabGroupChoices:v,setTabGroupChoices:w}=(0,l.U)(),[C,T]=(0,o.useState)(g),S=[],{blockElementScrollPositionUntilNextRender:I}=(0,c.o5)();if(null!=h){const e=v[h];null!=e&&e!==C&&b.some((t=>t.value===e))&&T(e)}const N=e=>{const t=e.currentTarget,n=S.indexOf(t),a=b[n].value;a!==C&&(I(t),T(a),null!=h&&w(h,String(a)))},x=e=>{var t;let n=null;switch(e.key){case"ArrowRight":{const t=S.indexOf(e.currentTarget)+1;n=S[t]??S[0];break}case"ArrowLeft":{const t=S.indexOf(e.currentTarget)-1;n=S[t]??S[S.length-1];break}}null==(t=n)||t.focus()};return o.createElement("div",{className:(0,r.Z)("tabs-container",u)},o.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.Z)("tabs",{"tabs--block":i},f)},b.map((e=>{let{value:t,label:n,attributes:i}=e;return o.createElement("li",(0,a.Z)({role:"tab",tabIndex:C===t?0:-1,"aria-selected":C===t,key:t,ref:e=>S.push(e),onKeyDown:x,onFocus:N,onClick:N},i,{className:(0,r.Z)("tabs__item",d,null==i?void 0:i.className,{"tabs__item--active":C===t})}),n??t)}))),n?(0,o.cloneElement)(k.filter((e=>e.props.value===C))[0],{className:"margin-top--md"}):o.createElement("div",{className:"margin-top--md"},k.map(((e,t)=>(0,o.cloneElement)(e,{key:t,hidden:e.props.value!==C})))))}function m(e){const t=(0,i.Z)();return o.createElement(p,(0,a.Z)({key:String(t)},e))}},7938:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>u,contentTitle:()=>l,default:()=>m,frontMatter:()=>s,metadata:()=>c,toc:()=>d});var a=n(7462),o=(n(7294),n(3905)),r=n(5488),i=n(8949);const s={id:"Clickhouse-Connector",title:"ClickHouse Connector",original:!0,sidebar_position:3},l=void 0,c={unversionedId:"connector/Clickhouse-Connector",id:"connector/Clickhouse-Connector",title:"ClickHouse Connector",description:"ClickHouse is a columnar database management system (DBMS) for online analytics (OLAP).",source:"@site/docs/connector/3-clickhouse.md",sourceDirName:"connector",slug:"/connector/Clickhouse-Connector",permalink:"/docs/connector/Clickhouse-Connector",draft:!1,editUrl:"https://github.com/apache/incubator-streampark-website/edit/dev/docs/connector/3-clickhouse.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{id:"Clickhouse-Connector",title:"ClickHouse Connector",original:!0,sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"JDBC Connector",permalink:"/docs/connector/Jdbc-Connector"},next:{title:"Apache Doris Connector",permalink:"/docs/connector/Doris-Connector"}},u={},d=[{value:"JDBC synchronous write",id:"jdbc-synchronous-write",level:2},{value:"Write in the normal way",id:"write-in-the-normal-way",level:3},{value:"Write with StreamPark",id:"write-with-streampark",level:3},{value:"configuration list",id:"configuration-list",level:4},{value:"Write to ClickHouse",id:"write-to-clickhouse",level:4},{value:"HTTP async write",id:"http-async-write",level:2},{value:"Write in the normal way",id:"write-in-the-normal-way-1",level:3},{value:"Write to ClickHouse",id:"write-to-clickhouse-1",level:3},{value:"Asynchronous write configuration and failure recovery component configuration",id:"asynchronous-write-configuration-and-failure-recovery-component-configuration",level:4},{value:"Write to clickhouse",id:"write-to-clickhouse-2",level:4},{value:"Other configuration",id:"other-configuration",level:2}],p={toc:d};function m(e){let{components:t,...n}=e;return(0,o.kt)("wrapper",(0,a.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://clickhouse.com/"},"ClickHouse")," is a columnar database management system (DBMS) for online analytics (OLAP).\nCurrently, Flink does not officially provide a connector for writing to ClickHouse and reading from ClickHouse.\nBased on the access form supported by ",(0,o.kt)("a",{parentName:"p",href:"https://clickhouse.com/docs/zh/interfaces/http/"},"ClickHouse - HTTP client"),"\nand ",(0,o.kt)("a",{parentName:"p",href:"https://clickhouse.com/docs/zh/interfaces/jdbc"},"JDBC driver"),", StreamPark encapsulates ClickHouseSink for writing data to ClickHouse in real-time."),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"ClickHouse")," writes do not support transactions, using JDBC write data to it could provide (AT_LEAST_ONCE) semanteme. Using the HTTP client to write asynchronously,\nit will retry the asynchronous write multiple times. The failed data will be written to external components (Kafka, MySQL, HDFS, HBase),\nthe data will be restored manually to achieve final data consistency."),(0,o.kt)("h2",{id:"jdbc-synchronous-write"},"JDBC synchronous write"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://clickhouse.com/"},"ClickHouse"),"provides a ",(0,o.kt)("a",{parentName:"p",href:"https://clickhouse.com/docs/zh/interfaces/jdbc/"},"JDBC driver"),",JDBC driver package of ClickHouse need to be import first"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-xml"},"<dependency>\n    <groupId>ru.yandex.clickhouse</groupId>\n    <artifactId>clickhouse-jdbc</artifactId>\n    <version>0.3.1</version>\n</dependency>\n")),(0,o.kt)("h3",{id:"write-in-the-normal-way"},"Write in the normal way"),(0,o.kt)("p",null,"The conventional way to create a clickhouse jdbc connection:"),(0,o.kt)(i.Z,{value:"Java",label:"Java",mdxType:"TabItem"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-java"},'import java.sql.Connection;\nimport java.sql.DriverManager;\nimport java.sql.SQLException;\n\npublic class ClickHouseUtil {\n    private static Connection connection;\n\n    public static Connection getConn(String host, int port, String database) throws SQLException, ClassNotFoundException {\n        Class.forName("ru.yandex.clickhouse.ClickHouseDriver");\n        String  address = "jdbc:clickhouse://" + host + ":" + port + "/" + database;\n        connection = DriverManager.getConnection(address);\n        return connection;\n    }\n\n    public static Connection getConn(String host, int port) throws SQLException, ClassNotFoundException {\n        return getConn(host,port,"default");\n    }\n    public static Connection getConn() throws SQLException, ClassNotFoundException {\n        return getConn("node-01",8123);\n    }\n    public void close() throws SQLException {\n        connection.close();\n    }\n}\n'))),(0,o.kt)("p",null,"The method of splicing various parameters into the request url is cumbersome and hard-coded, which is very inflexible."),(0,o.kt)("h3",{id:"write-with-streampark"},"Write with StreamPark"),(0,o.kt)("p",null,"To access ",(0,o.kt)("inlineCode",{parentName:"p"},"ClickHouse")," data with ",(0,o.kt)("inlineCode",{parentName:"p"},"StreamPark"),", you only need to define the configuration file in the specified format and then write code.\nThe configuration and code are as follows. The configuration of ",(0,o.kt)("inlineCode",{parentName:"p"},"ClickHose JDBC")," in ",(0,o.kt)("inlineCode",{parentName:"p"},"StreamPark")," is in the configuration list, and the sample running program is scala"),(0,o.kt)("h4",{id:"configuration-list"},"configuration list"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"clickhouse:\n  sink:\n    jdbcUrl: jdbc:clickhouse://127.0.0.1:8123,192.168.1.2:8123\n    socketTimeout: 3000000\n    database: test\n    user: $user\n    password: $password\n    targetTable: orders(userId,siteId)\n    batch:\n      size: 1000\n      delaytime: 6000000\n")),(0,o.kt)("h4",{id:"write-to-clickhouse"},"Write to ClickHouse"),(0,o.kt)(r.Z,{mdxType:"Tabs"},(0,o.kt)(i.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-scala"},'import org.apache.streampark.flink.core.scala.FlinkStreaming\nimport org.apache.streampark.flink.core.scala.sink.ClickHouseSink\nimport org.apache.flink.api.scala._\n\nobject ClickHouseSinkApp extends FlinkStreaming {\n\n  override def handle(): Unit = {\n    val createTable =\n      """\n        |create TABLE test.orders(\n        |userId UInt16,\n        |orderId UInt16,\n        |siteId UInt8,\n        |cityId UInt8,\n        |orderStatus UInt8,\n        |price Float64,\n        |quantity UInt8,\n        |timestamp UInt16\n        |)ENGINE = TinyLog;\n        |""".stripMargin\n\n    val source = context.addSource(new TestSource)\n\n\n     ClickHouseSink().syncSink[TestEntity](source)(x => {\n         s"(${x.userId},${x.siteId})"\n     }).setParallelism(1)\n  }\n\n}\n\nclass Order(val marketId: String, val timestamp: String) extends Serializable\n')))),(0,o.kt)("admonition",{title:"hint",type:"tip"},(0,o.kt)("p",{parentName:"admonition"},"ClickHouse can support balanced writing of multiple nodes, you only need to configure writable nodes in JDBC URL\nSince ClickHouse has a relatively high delay for single insertion, it is recommended to set the batch.\nsize to insert data in batches to improve performance. At the same time, to improve real-time performance,\nit supports batch writing according to data volume or interval time.\nIn the implementation of ClickHouseSink, if the number of the last batch of data is less than BatchSize, the remaining data will be inserted when the connection is closed.")),(0,o.kt)("h2",{id:"http-async-write"},"HTTP async write"),(0,o.kt)("p",null,"In the case of a small amount of data, you can use JDBC to connect and write data. In actual production\uff0cis more using async HTTP to write data more efficiently and quickly."),(0,o.kt)("h3",{id:"write-in-the-normal-way-1"},"Write in the normal way"),(0,o.kt)("p",null,"Clickhouse INSERT must insert data through the POST method. The general operation is as follows:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"$ echo 'INSERT INTO t VALUES (1),(2),(3)' | curl 'http://localhost:8123/' --data-binary @-\n")),(0,o.kt)("p",null,"The operation of the above method is relatively simple. Sure java could also be used for writing. StreamPark adds many functions to the http post writing method,\nincluding encapsulation enhancement, adding cache, asynchronous writing, failure retry, and data backup after reaching the retry threshold\uff0c\nTo external components (kafka, mysql, hdfs, hbase), etc., the above functions only need to define the configuration file in the prescribed format,\nand write the code."),(0,o.kt)("h3",{id:"write-to-clickhouse-1"},"Write to ClickHouse"),(0,o.kt)("p",null,"The configuration of ",(0,o.kt)("inlineCode",{parentName:"p"},"ClickHose JDBC")," in ",(0,o.kt)("inlineCode",{parentName:"p"},"StreamPark")," is in the configuration list, and the sample running program is scala, as follows:\nasynchttpclient is used as an HTTP asynchronous client for writing. first, import the jar of asynchttpclient"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-xml"},"\x3c!--clickhouse async need asynchttpclient --\x3e\n<dependency>\n    <groupId>org.asynchttpclient</groupId>\n    <artifactId>async-http-client</artifactId>\n    <optional>true</optional>\n</dependency>\n")),(0,o.kt)("h4",{id:"asynchronous-write-configuration-and-failure-recovery-component-configuration"},"Asynchronous write configuration and failure recovery component configuration"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"\nclickhouse:\n  sink:\n    hosts: 127.0.0.1:8123,192.168.1.2:8123\n    socketTimeout: 3000000\n    database: test\n    user: $user\n    password: $password\n    targetTable: test.orders(userId,siteId)\n    batch:\n      size: 1\n      delaytime: 60000\n    threshold:\n      bufferSize: 10\n      #      Concurrent number of asynchronous writes\n      numWriters: 4\n      #      cache queue size\n      queueCapacity: 100\n      delayTime: 10\n      requestTimeout: 600\n      retries: 1\n      #      success response code\n      successCode: 200\n    failover:\n      table: chfailover\n      #     After reaching the maximum number of failed writes, the components of the data backup\n      storage: kafka #kafka|mysql|hbase|hdfs\n      mysql:\n        driverClassName: com.mysql.cj.jdbc.Driver\n        jdbcUrl: jdbc:mysql://localhost:3306/test?useSSL=false&allowPublicKeyRetrieval=true\n        username: $user\n        password: $pass\n      kafka:\n        bootstrap.servers: localhost:9092\n        topic: test1\n        group.id: user_01\n        auto.offset.reset: latest\n      hbase:\n        zookeeper.quorum: localhost\n        zookeeper.property.clientPort: 2181\n      hdfs:\n        path: /data/chfailover\n        namenode: hdfs://localhost:8020\n        user: hdfs\n")),(0,o.kt)("h4",{id:"write-to-clickhouse-2"},"Write to clickhouse"),(0,o.kt)(r.Z,{mdxType:"Tabs"},(0,o.kt)(i.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-scala"},'\nimport org.apache.streampark.flink.core.scala.FlinkStreaming\nimport org.apache.streampark.flink.core.scala.sink.ClickHouseSink\nimport org.apache.flink.api.scala._\n\nobject ClickHouseSinkApp extends FlinkStreaming {\n\n  override def handle(): Unit = {\n    val createTable =\n      """\n        |create TABLE test.orders(\n        |userId UInt16,\n        |orderId UInt16,\n        |siteId UInt8,\n        |cityId UInt8,\n        |orderStatus UInt8,\n        |price Float64,\n        |quantity UInt8,\n        |timestamp UInt16\n        |)ENGINE = TinyLog;\n        |""".stripMargin\n\n    println(createTable)\n\n    val source = context.addSource(new TestSource)\n\n\n    // asynchronous write\n    ClickHouseSink().sink[TestEntity](source)(x => {\n      s"(${x.userId},${x.siteId})"\n    }).setParallelism(1)\n\n  }\n\n}\n\nclass Order(val marketId: String, val timestamp: String) extends Serializable\n')))),(0,o.kt)("admonition",{title:"warn",type:"info"},(0,o.kt)("p",{parentName:"admonition"},"Due to the high latency of single insertion of ClickHouse, partitions will be merged too frequently by the ClickHouse server,\nbecause of frequent writing of small data.It is recommended to use the asynchronous submission method and set a reasonable threshold to improve performance."),(0,o.kt)("p",{parentName:"admonition"},"Since ClickHouse will re-add data to the cache queue when asynchronous writing fails, it may cause the same window of data to be written in two batches.\nIt is recommended to fully test the stability of ClickHouse in scenarios with high real-time requirements."),(0,o.kt)("p",{parentName:"admonition"},"After the asynchronous write data reaches the maximum retry value, the data will be backed up to the external component,\nand the component connection will be initialized at this time. It is recommended to ensure the availability of the failover component.")),(0,o.kt)("h2",{id:"other-configuration"},"Other configuration"),(0,o.kt)("p",null,"All other configurations must comply with the ",(0,o.kt)("strong",{parentName:"p"},"ClickHouseDataSource")," connection pool configuration.\nFor specific configurable items and the role of each parameter, please refer to the ",(0,o.kt)("inlineCode",{parentName:"p"},"ClickHouse-JDBC")," ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/ClickHouse/clickhouse-jdbc"},"official website documentation"),"."))}m.isMDXComponent=!0}}]);