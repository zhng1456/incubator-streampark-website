(self.webpackChunkapache_streampark_website=self.webpackChunkapache_streampark_website||[]).push([[6456],{3905:(e,t,a)=>{"use strict";a.d(t,{Zo:()=>m,kt:()=>u});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),c=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},m=function(e){var t=c(e.components);return n.createElement(s.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,m=i(e,["components","mdxType","originalType","parentName"]),d=c(a),u=r,h=d["".concat(s,".").concat(u)]||d[u]||p[u]||o;return a?n.createElement(h,l(l({ref:t},m),{},{components:a})):n.createElement(h,l({ref:t},m))}));function u(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,l=new Array(o);l[0]=d;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i.mdxType="string"==typeof e?e:r,l[1]=i;for(var c=2;c<o;c++)l[c]=a[c];return n.createElement.apply(null,l)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},6954:(e,t,a)=>{"use strict";a.d(t,{OC:()=>y,k:()=>f,Xv:()=>v,_3:()=>g,zS:()=>k,Lh:()=>u,fx:()=>p,HL:()=>d,rK:()=>b,LP:()=>h});var n=a(7294);const r={option:[{opt:"-t",longOpt:"target",desc:"Deployment mode(only support yarn-per-job,application)",deprecated:!1,value:" yarn-per-job | application "},{opt:"-d",longOpt:"detached",desc:"run as detached mode",deprecated:!1,value:"true | false"},{opt:"-n",longOpt:"allowNonRestoredState",desc:"allow to skip savepoint state that cannot be restored",deprecated:!1,value:"true | false"},{opt:"-sae",longOpt:"shutdownOnAttachedExit",desc:"If the job is submitted in attached, when job cancel close cluster",deprecated:!1,value:"true | false"},{opt:"-m",longOpt:"jobmanager",desc:"Address of the JobManager to which to connect",deprecated:!1,value:"yarn-cluster | address"},{opt:"-p",longOpt:"parallelism",desc:"Program parallelism",deprecated:!0,value:"int"},{opt:"-c",longOpt:"class",desc:'Class with the program entry point ("main()" method)',deprecated:!0,value:"String"}],property:[{name:"$internal.application.main",desc:'Class with the program entry point ("main()" method)',required:!0},{name:"pipeline.name",desc:"Job name",required:!0},{name:"yarn.application.queue",desc:"YARN queue",required:!1},{name:"taskmanager.numberOfTaskSlots",desc:"Taskmanager slot number",required:!1},{name:"parallelism.default",desc:"Program parallelism",required:!1}],memory:[{group:"JM heap Memory",name:"jobmanager.memory.heap.size",desc:"JVM Heap Memory size for JobManager. The minimum recommended JVM Heap size is 128.000mb (134217728 bytes)."},{group:"JM Off-heap Memory",name:"jobmanager.memory.off-heap.size",desc:"Off-heap Memory size for JobManager. This option covers all off-heap memory usage including direct and native memory allocation. The JVM direct memory limit of the JobManager process (-XX:MaxDirectMemorySize) will be set to this value if the limit is enabled by jobmanager.memory.enable-jvm-direct-memory-limit"},{group:"JVM Metaspace",name:"jobmanager.memory.jvm-metaspace.size",desc:"JVM Metaspace Size for the JobManager."},{group:"JVM Size",name:"jobmanager.memory.jvm-overhead.min",desc:"Min JVM Overhead size for the JobManager. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value."},{group:"JVM Size",name:"jobmanager.memory.jvm-overhead.max",desc:"Max JVM Overhead size for the JobManager. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value."},{group:"JVM Size",name:"jobmanager.memory.jvm-overhead.fraction",desc:"Fraction of Total Process Memory to be reserved for JVM Overhead. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value."},{group:"Framework Heap Memory",name:"taskmanager.memory.framework.heap.size",desc:"Framework Heap Memory size for TaskExecutors. This is the size of JVM heap memory reserved for TaskExecutor framework, which will not be allocated to task slots."},{group:"Task Heap Memory",name:"taskmanager.memory.task.heap.size",desc:"Task Heap Memory size for TaskExecutors. This is the size of JVM heap memory reserved for tasks. If not specified, it will be derived as Total Flink Memory minus Framework Heap Memory, Framework Off-Heap Memory, Task Off-Heap Memory, Managed Memory and Network Memory."},{group:"Managed memory",name:"taskmanager.memory.managed.size",desc:"Managed Memory size for TaskExecutors. This is the size of off-heap memory managed by the memory manager, reserved for sorting, hash tables, caching of intermediate results and RocksDB state backend. Memory consumers can either allocate memory from the memory manager in the form of MemorySegments, or reserve bytes from the memory manager and keep their memory usage within that boundary. If unspecified, it will be derived to make up the configured fraction of the Total Flink Memory."},{group:"Managed memory",name:"taskmanager.memory.managed.fraction",desc:"Fraction of Total Flink Memory to be used as Managed Memory, if Managed Memory size is not explicitly specified."},{group:"Framework off-heap memory",name:"taskmanager.memory.framework.off-heap.size",desc:"Framework Off-Heap Memory size for TaskExecutors. This is the size of off-heap memory (JVM direct memory and native memory) reserved for TaskExecutor framework, which will not be allocated to task slots. The configured value will be fully counted when Flink calculates the JVM max direct memory size parameter."},{group:"Taskmanager off-heap memory",name:"taskmanager.memory.task.off-heap.size",desc:"Task Off-Heap Memory size for TaskExecutors. This is the size of off heap memory (JVM direct memory and native memory) reserved for tasks. The configured value will be fully counted when Flink calculates the JVM max direct memory size parameter."},{group:"JVM Metaspace",name:"taskmanager.memory.jvm-metaspace.size",desc:"JVM Metaspace Size for the TaskExecutors."}],totalMem:[{group:"Flink total memory",tm:"taskmanager.memory.flink.size",jm:"jobmanager.memory.flink.size"},{group:"Flink process total memory",tm:"taskmanager.memory.process.size",jm:"jobmanager.memory.process.size"}],checkpoints:[{name:"execution.checkpointing.interval",desc:"Interval period of checkpoint",value:"Duration"},{name:"execution.checkpointing.timeout",desc:"timeout",value:"Duration"},{name:"execution.checkpointing.mode",desc:"semantics",value:" EXACTLY_ONCE | AT_LEAST_ONCE "},{name:"execution.checkpointing.unaligned",desc:"unaligned",value:"true | false"}],backend:[{name:"state.backend",desc:"Type of backend storage",value:"hashmap | rocksdb",mode:""},{name:"state.checkpoint-storage",desc:"The checkpoint storage implementation to be used to checkpoint state.",value:"jobmanager | filesystem ",mode:""},{name:"state.backend.incremental",desc:"Whether to enable increment",value:" true | false",mode:"rocksdb"}],fixedDelay:[{name:"attempts",desc:"Number of Flink attempts to restart",value:"3"},{name:"delay",desc:"Specify how long to restart after the task fails",value:"none | s | m | min | h | d"}],failureRate:[{name:"max-failures-per-interval",desc:"Maximum number of restarts in given time interval before failing a job",value:"3"},{name:"failure-rate-interval",desc:"Time interval for measuring failure",value:"\u65e0 | s | m | min | h | d"},{name:"delay",desc:"Delay between two consecutive restart attempts",value:"\u65e0 | s | m | min | h | d"}],tables:[{name:"planner",desc:"Table Planner",value:"blink | old | any"},{name:"mode",desc:"Table Mode",value:"streaming | batch"},{name:"catalog",desc:"Catalog,Specifies that the will be used during initialization",value:""},{name:"database",desc:"Database,Specifies that the will be used during initialization",value:""}],envs:[{name:"\u64cd\u4f5c\u7cfb\u7edf",version:"Linux",required:!0,other:"\u4e0d\u652f\u6301Window\u7cfb\u7edf"},{name:"JAVA",version:"1.8+",required:!0,other:null},{name:"Maven",version:"3+",required:!1,other:"\u90e8\u7f72\u673a\u5668\u53ef\u9009\u5b89\u88c5Maven(\u9879\u76ee\u7f16\u8bd1\u4f1a\u7528\u5230)"},{name:"Node.js",version:"",required:!0,other:"NodeJs\u76f8\u5173\u73af\u5883"},{name:"Flink",version:"1.12.0+",required:!0,other:"\u7248\u672c\u5fc5\u987b\u662f1.12.x\u6216\u4ee5\u4e0a\u7248\u672c,scala\u7248\u672c\u5fc5\u987b\u662f2.11"},{name:"Hadoop",version:"2+",required:!1,other:"\u53ef\u9009,\u5982\u679con yarn\u5219\u9700\u8981hadoop\u73af\u5883,\u5e76\u4e14\u914d\u7f6e\u597d\u76f8\u5173\u73af\u5883\u53d8\u91cf"},{name:"MySQL",version:"5.6+",required:!1,other:"\u90e8\u7f72\u673a\u5668\u6216\u8005\u5176\u4ed6\u673a\u5668\u5b89\u88c5MySQL"},{name:"Python",version:"2+",required:!1,other:"\u53ef\u9009,\u706b\u7130\u56fe\u529f\u80fd\u4f1a\u7528\u5230Python"},{name:"Perl",version:"5.16.3+",required:!1,other:"\u53ef\u9009,\u706b\u7130\u56fe\u529f\u80fd\u4f1a\u7528\u5230Perl"}]};let o=null,l=0;function i(e){o||(o=window.document.createElement("div"),o.setAttribute("class","cpt-toast-wrapper"),window.document.body.append(o));let t=""+Date.now()+l++,a=window.document.createElement("div");a.setAttribute("id",t),a.innerHTML=`<div class="cpt-toast"><span class="${e.icon}">${e.msg}</span></div>`,o.append(a),setTimeout((()=>{window.document.getElementById(t).remove()}),e.time||1e3)}const s={success(e,t){i({msg:e,time:t,icon:"success"})},error(e,t){i({msg:e,time:t,icon:"error"})},info(e,t){i({msg:e,time:t,icon:"info"})}};var c=a(4855);const m=()=>{s.success("Copy succeeded \ud83c\udf89")},p=()=>n.createElement("div",null,n.createElement("table",{className:"table-data",style:{width:"100%",display:"inline-table"}},n.createElement("thead",null,n.createElement("tr",null,n.createElement("td",{style:{width:"80px"}},"Short Param"),n.createElement("td",null,'Full Param(prefix"--")'),n.createElement("td",{style:{width:"60px"}},"Effective"),n.createElement("td",null,"Value & Type"),n.createElement("td",null,"Description"))),n.createElement("tbody",null,r.option.map(((e,t)=>n.createElement("tr",{key:t},n.createElement("td",null,e.opt),n.createElement("td",null,e.longOpt),n.createElement("td",null,e.deprecated?n.createElement("span",{className:"icon-times"}):n.createElement("span",{className:"icon-check"})),n.createElement("td",null,e.value),n.createElement("td",null,e.desc))))))),d=()=>n.createElement("div",null,n.createElement("table",{className:"table-data",style:{width:"100%",display:"inline-table"}},n.createElement("thead",null,n.createElement("tr",null,n.createElement("td",null,"Key"),n.createElement("td",null,"Description"),n.createElement("td",null,"Required"))),n.createElement("tbody",null,r.property.map(((e,t)=>n.createElement("tr",{key:t},n.createElement("td",null,n.createElement("span",{className:"label-info"},e.name),n.createElement(c.CopyToClipboard,{text:e.name,onCopy:()=>m()},n.createElement("i",{className:"icon-copy"}))),n.createElement("td",null,e.desc),n.createElement("td",null,e.required?n.createElement("span",{className:"icon-toggle-on",title:"\u5fc5\u987b"}):n.createElement("span",{className:"icon-toggle-off",title:"\u53ef\u9009"})))))))),u=()=>n.createElement("div",null,n.createElement("table",{className:"table-data",style:{width:"100%",display:"inline-table"}},n.createElement("thead",null,n.createElement("tr",null,n.createElement("td",{style:{width:"380px"}},"Key"),n.createElement("td",null,"Description"))),n.createElement("tbody",null,r.memory.map(((e,t)=>n.createElement("tr",{key:t},n.createElement("td",null,n.createElement("span",{className:"label-info"},e.name),n.createElement(c.CopyToClipboard,{text:e.name,onCopy:()=>m()},n.createElement("i",{className:"icon-copy"}))),n.createElement("td",null,e.desc))))))),h=()=>n.createElement("div",null,n.createElement("table",{className:"table-data",style:{width:"100%",display:"inline-table"}},n.createElement("thead",null,n.createElement("tr",null,n.createElement("td",null,"Item"),n.createElement("td",null,"TaskManager Config"),n.createElement("td",null,"JobManager Config"))),n.createElement("tbody",null,r.totalMem.map(((e,t)=>n.createElement("tr",{key:t},n.createElement("td",null,e.group),n.createElement("td",null,n.createElement("span",{className:"label-info"},e.tm),n.createElement(c.CopyToClipboard,{text:e.tm,onCopy:()=>m()},n.createElement("i",{className:"icon-copy"}))),n.createElement("td",null,n.createElement("span",{className:"label-info"},e.jm),n.createElement(c.CopyToClipboard,{text:e.jm,onCopy:()=>m()},n.createElement("i",{className:"icon-copy"}))))))))),f=()=>n.createElement("div",null,n.createElement("table",{className:"table-data",style:{width:"100%",display:"inline-table"}},n.createElement("thead",null,n.createElement("tr",null,n.createElement("td",null,"Item"),n.createElement("td",null,"Description"),n.createElement("td",null,"Value | Type"))),n.createElement("tbody",null,r.checkpoints.map(((e,t)=>n.createElement("tr",{key:t},n.createElement("td",null,e.name),n.createElement("td",null,e.desc),n.createElement("td",null,e.value))))))),y=()=>n.createElement("div",null,n.createElement("table",{className:"table-data",style:{width:"100%",display:"inline-table"}},n.createElement("thead",null,n.createElement("tr",null,n.createElement("td",null,"Item"),n.createElement("td",null,"Description"),n.createElement("td",null,"Value | Type"),n.createElement("td",null,"Effective rules"))),n.createElement("tbody",null,r.backend.map(((e,t)=>n.createElement("tr",{key:t},n.createElement("td",null,n.createElement("span",{className:"label-info"},e.name),n.createElement(c.CopyToClipboard,{text:e.name,onCopy:()=>m()},n.createElement("i",{className:"icon-copy"}))),n.createElement("td",null,e.desc),n.createElement("td",null,e.value),n.createElement("td",null,e.mode))))))),k=()=>n.createElement("div",null,n.createElement("table",{className:"table-data",style:{width:"100%",display:"inline-table"}},n.createElement("thead",null,n.createElement("tr",null,n.createElement("td",null,"Item"),n.createElement("td",null,"Description"),n.createElement("td",null,"Value | Unit"))),n.createElement("tbody",null,r.fixedDelay.map(((e,t)=>n.createElement("tr",{key:t},n.createElement("td",null,n.createElement("span",{className:"label-info"},e.name),n.createElement(c.CopyToClipboard,{text:e.name,onCopy:()=>m()},n.createElement("i",{className:"icon-copy"}))),n.createElement("td",null,e.desc),n.createElement("td",null,e.value))))))),g=()=>n.createElement("div",null,n.createElement("table",{className:"table-data",style:{width:"100%",display:"inline-table"}},n.createElement("thead",null,n.createElement("tr",null,n.createElement("td",null,"Item"),n.createElement("td",null,"Description"),n.createElement("td",null,"Value | Unit"))),n.createElement("tbody",null,r.failureRate.map(((e,t)=>n.createElement("tr",{key:t},n.createElement("td",null,n.createElement("span",{className:"label-info"},e.name),n.createElement(c.CopyToClipboard,{text:e.name,onCopy:()=>m()},n.createElement("i",{className:"icon-copy"}))),n.createElement("td",null,e.desc),n.createElement("td",null,e.value))))))),b=()=>n.createElement("div",null,n.createElement("table",{className:"table-data",style:{width:"100%",display:"inline-table"}},n.createElement("thead",null,n.createElement("tr",null,n.createElement("td",null,"Item"),n.createElement("td",null,"Description"),n.createElement("td",null,"Value"))),n.createElement("tbody",null,r.tables.map(((e,t)=>n.createElement("tr",{key:t},n.createElement("td",null,n.createElement("span",{className:"label-info"},e.name),n.createElement(c.CopyToClipboard,{text:e.name,onCopy:()=>m()},n.createElement("i",{className:"icon-copy"}))),n.createElement("td",null,e.desc),n.createElement("td",null,e.value))))))),v=()=>n.createElement("div",null,n.createElement("table",{className:"table-data",style:{width:"100%",display:"inline-table"}},n.createElement("thead",null,n.createElement("tr",null,n.createElement("td",null,"Item"),n.createElement("td",null,"Version"),n.createElement("td",null,"Required"),n.createElement("td",null,"Other"))),n.createElement("tbody",null,r.envs.map(((e,t)=>n.createElement("tr",{key:t},n.createElement("td",null,n.createElement("span",{className:"label-info"},e.name)),n.createElement("td",null,e.version),n.createElement("td",null,e.required?n.createElement("span",{className:"icon-toggle-on",title:"Required"}):n.createElement("span",{className:"icon-toggle-off",title:"Optional"})),n.createElement("td",null,e.other)))))))},1398:(e,t,a)=>{"use strict";a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>d,frontMatter:()=>l,metadata:()=>s,toc:()=>m});var n=a(7462),r=(a(7294),a(3905)),o=a(6954);const l={id:"deployment",title:"Platform deployment",sidebar_position:1},i=void 0,s={unversionedId:"user-guide/deployment",id:"user-guide/deployment",title:"Platform deployment",description:"The overall component stack structure of StreamPark is as follows. It consists of two major parts: streampark-core and streampark-console. streampark-console is a very important module, positioned as a integrated real-time data platform,  streaming data warehouse Platform, Low Code, Flink & Spark task hosting platform, can better manage Flink tasks, integrate project compilation, publishing, parameter configuration, startup, savepoint, flame graph ( flame graph ), Flink SQL, monitoring and many other functions are integrated into one, which greatly simplifies the daily operation and maintenance of Flink tasks and integrates many best practices. Its ultimate goal is to create a one-stop big data solution that integrates real-time data warehouses and batches",source:"@site/docs/user-guide/1-deployment.md",sourceDirName:"user-guide",slug:"/user-guide/deployment",permalink:"/docs/user-guide/deployment",draft:!1,editUrl:"https://github.com/apache/incubator-streampark-website/edit/dev/docs/user-guide/1-deployment.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{id:"deployment",title:"Platform deployment",sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"What is StreamPark",permalink:"/docs/intro"},next:{title:"Quick start",permalink:"/docs/user-guide/quick-start"}},c={},m=[{value:"Environmental requirements",id:"environmental-requirements",level:2},{value:"Hadoop",id:"hadoop",level:3},{value:"Kubernetes",id:"kubernetes",level:3},{value:"Build &amp; Deploy",id:"build--deploy",level:2},{value:"Environmental requirements",id:"environmental-requirements-1",level:3},{value:"Compile and package",id:"compile-and-package",level:3},{value:"Automatic packaging",id:"automatic-packaging",level:4},{value:"Manual packaging",id:"manual-packaging",level:4},{value:"mixed packaging",id:"mixed-packaging",level:5},{value:"Independent packaging",id:"independent-packaging",level:5},{value:"1. Backend compilation",id:"1-backend-compilation",level:6},{value:"2. Front-end packaging",id:"2-front-end-packaging",level:6},{value:"Deploy backend",id:"deploy-backend",level:3},{value:"Initialize table structure",id:"initialize-table-structure",level:5},{value:"Initialize table data",id:"initialize-table-data",level:5},{value:"Modify the configuration",id:"modify-the-configuration",level:5},{value:"Create a new database <code>streampark</code>",id:"create-a-new-database-streampark",level:6},{value:"Modify connection information",id:"modify-connection-information",level:6},{value:"Modify workspace",id:"modify-workspace",level:6},{value:"Start the backend",id:"start-the-backend",level:5},{value:"Deploy frontend",id:"deploy-frontend",level:3},{value:"Environmental preparation",id:"environmental-preparation",level:5},{value:"Release",id:"release",level:5},{value:"1. Copy the dist to the deployment server",id:"1-copy-the-dist-to-the-deployment-server",level:6},{value:"2. Copy the streampark.js file to the project deployment directory",id:"2-copy-the-streamparkjs-file-to-the-project-deployment-directory",level:6},{value:"3. Modify the service port",id:"3-modify-the-service-port",level:6},{value:"log in system",id:"log-in-system",level:3},{value:"System Configuration",id:"system-configuration",level:2},{value:"Flink Home",id:"flink-home",level:3},{value:"Maven Home",id:"maven-home",level:3},{value:"StreamPark Env",id:"streampark-env",level:3},{value:"Email",id:"email",level:3}],p={toc:m};function d(e){let{components:t,...l}=e;return(0,r.kt)("wrapper",(0,n.Z)({},p,l,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"The overall component stack structure of StreamPark is as follows. It consists of two major parts: streampark-core and streampark-console. streampark-console is a very important module, positioned as a ",(0,r.kt)("strong",{parentName:"p"},"integrated real-time data platform"),", ",(0,r.kt)("strong",{parentName:"p"}," streaming data warehouse Platform"),", ",(0,r.kt)("strong",{parentName:"p"},"Low Code"),", ",(0,r.kt)("strong",{parentName:"p"},"Flink & Spark task hosting platform"),", can better manage Flink tasks, integrate project compilation, publishing, parameter configuration, startup, savepoint, flame graph ( flame graph ), Flink SQL, monitoring and many other functions are integrated into one, which greatly simplifies the daily operation and maintenance of Flink tasks and integrates many best practices. Its ultimate goal is to create a one-stop big data solution that integrates real-time data warehouses and batches"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"StreamPark Archite",src:a(9844).Z,width:"2428",height:"1236"})),(0,r.kt)("p",null,"streampark-console provides an out-of-the-box installation package. Before installation, there are some requirements for the environment. The specific requirements are as follows:"),(0,r.kt)("h2",{id:"environmental-requirements"},"Environmental requirements"),(0,r.kt)(o.Xv,{mdxType:"ClientEnvs"}),(0,r.kt)("admonition",{title:"Notice",type:"tip"},(0,r.kt)("p",{parentName:"admonition"},"The versions before (including) StreamPark 1.2.2 only support ",(0,r.kt)("inlineCode",{parentName:"p"},"scala 2.11"),". Do not check the corresponding ",(0,r.kt)("inlineCode",{parentName:"p"},"scala")," version when using ",(0,r.kt)("inlineCode",{parentName:"p"},"flink"),"\nVersions after (including) 1.2.3, support both ",(0,r.kt)("inlineCode",{parentName:"p"},"scala 2.11")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"scala 2.12")," versions")),(0,r.kt)("p",null,"At present, StreamPark has released tasks for Flink, and supports both ",(0,r.kt)("inlineCode",{parentName:"p"},"Flink on YARN")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"Flink on Kubernetes")," modes."),(0,r.kt)("h3",{id:"hadoop"},"Hadoop"),(0,r.kt)("p",null,"To use ",(0,r.kt)("inlineCode",{parentName:"p"},"Flink on YARN"),", you need to install and configure Hadoop-related environment variables in the deployed cluster. For example, if you installed the hadoop environment based on CDH,\nRelated environment variables can refer to the following configuration:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"export HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop #hadoop installation manual\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport HIVE_HOME=$HADOOP_HOME/../hive\nexport HBASE_HOME=$HADOOP_HOME/../hbase\nexport HADOOP_HDFS_HOME=$HADOOP_HOME/../hadoop-hdfs\nexport HADOOP_MAPRED_HOME=$HADOOP_HOME/../hadoop-mapreduce\nexport HADOOP_YARN_HOME=$HADOOP_HOME/../hadoop-yarn\n")),(0,r.kt)("h3",{id:"kubernetes"},"Kubernetes"),(0,r.kt)("p",null,"Using ",(0,r.kt)("inlineCode",{parentName:"p"},"Flink on Kubernetes")," requires additional deployment/or use of an existing Kubernetes cluster, please refer to the entry: ",(0,r.kt)("a",{parentName:"p",href:"/docs/flink-k8s/k8s-dev"},(0,r.kt)("strong",{parentName:"a"},"StreamPark Flink-K8s Integration Support")),"."),(0,r.kt)("h2",{id:"build--deploy"},"Build & Deploy"),(0,r.kt)("p",null,"You can directly download the compiled ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/apache/streampark/releases"},(0,r.kt)("strong",{parentName:"a"},"release package"))," (recommended), or you can choose to manually compile and install. The manual compilation and installation steps are as follows:"),(0,r.kt)("h3",{id:"environmental-requirements-1"},"Environmental requirements"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Maven 3.6+"),(0,r.kt)("li",{parentName:"ul"},"npm 7.11.2 ( ",(0,r.kt)("a",{parentName:"li",href:"https://nodejs.org/en/"},"https://nodejs.org/en/")," )"),(0,r.kt)("li",{parentName:"ul"},"pnpm (npm install -g pnpm)"),(0,r.kt)("li",{parentName:"ul"},"JDK 1.8+")),(0,r.kt)("h3",{id:"compile-and-package"},"Compile and package"),(0,r.kt)("h4",{id:"automatic-packaging"},"Automatic packaging"),(0,r.kt)("p",null,"Starting from StreamPark 1.2.3+, an automatic compilation script ",(0,r.kt)("inlineCode",{parentName:"p"},"build.sh")," is provided. Execute and run the script and select the next step as required to complete the compilation. If the version before StreamPark 1.2.3 can be skipped, see directly Manually package some documents"),(0,r.kt)("video",{src:"http://assets.streamxhub.com/streamx-build.mp4",controls:"controls",width:"100%",height:"100%"}),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"\n./build.sh\n\n")),(0,r.kt)("h4",{id:"manual-packaging"},"Manual packaging"),(0,r.kt)("p",null,"From 1.2.1 and later versions, StreamPark supports ",(0,r.kt)("strong",{parentName:"p"},"mixed packaging")," and ",(0,r.kt)("strong",{parentName:"p"},"independent packaging")," two modes for users to choose. If you manually package and deploy, you need to pay attention to the specific operations of each packaging method:"),(0,r.kt)("h5",{id:"mixed-packaging"},"mixed packaging"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"git clone git@github.com:apache/incubator-streampark.git streampark\ncd streampark\nmvn clean install -DskipTests -Dscala.version=2.11.12 -Dscala.binary.version=2.11 -Pwebapp\n")),(0,r.kt)("admonition",{title:"special attention",type:"danger"},(0,r.kt)("span",{style:{color:"red"}},"**-Pwebapp**")," This parameter is required in front-end and back-end mixed packaging mode"),(0,r.kt)("h5",{id:"independent-packaging"},"Independent packaging"),(0,r.kt)("h6",{id:"1-backend-compilation"},"1. Backend compilation"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"git clone git@github.com:apache/incubator-streampark.git streampark\ncd streampark\nmvn clean install -Dscala.version=2.12.8 -Dscala.binary.version=2.12\n")),(0,r.kt)("h6",{id:"2-front-end-packaging"},"2. Front-end packaging"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"2.1 Modify base api")),(0,r.kt)("p",null,"In a project where the front-end and back-end are independently compiled and deployed, the front-end project needs to know the base api of the back-end service so that the front-end and back-end can work together. Therefore, before compiling, we need to specify the base api of the back-end service and modify streampark-console-webapp/. ",(0,r.kt)("inlineCode",{parentName:"p"},"VUE_APP_BASE_API")," in env.production can"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"vi streampark/streampark-console/streampark-console-webapp/.env.production\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"2.2 Compile")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"git clone git@github.com:apache/incubator-streampark.git streampark\ncd streampark/streampark-console/streampark-console-webapp\nnpm install\nnpm run build\n")),(0,r.kt)("admonition",{title:"pay attention",type:"danger"},(0,r.kt)("p",{parentName:"admonition"},"Pay attention to the parameters carried when each different version is compiled,\nIn versions after StreamPark 1.2.3 (included), the ",(0,r.kt)("inlineCode",{parentName:"p"},"-Dscala.version")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"-Dscala.binary.version")," parameters are required"),(0,r.kt)("p",{parentName:"admonition"},"Scala 2.11 is compiled, and the relevant scala version specification information is as follows:"),(0,r.kt)("pre",{parentName:"admonition"},(0,r.kt)("code",{parentName:"pre"},"-Dscala.version=2.11.12 -Dscala.binary.version=2.11\n")),(0,r.kt)("p",{parentName:"admonition"},"Scala 2.12 is compiled, and the relevant scala version specification information is as follows:"),(0,r.kt)("pre",{parentName:"admonition"},(0,r.kt)("code",{parentName:"pre"},"-Dscala.version=2.12.7 -Dscala.binary.version=2.12\n"))),(0,r.kt)("h3",{id:"deploy-backend"},"Deploy backend"),(0,r.kt)("p",null,"After the installation is complete, you will see the final project file, located in ",(0,r.kt)("inlineCode",{parentName:"p"},"streampark/streampark-console/streampark-console-service/target/streampark-console-service-${version}-bin.tar.gz"),", the installation directory after unpacking as follows"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-textmate"},".\nstreampark-console-service-1.2.1\n\u251c\u2500\u2500 bin\n\u2502    \u251c\u2500\u2500 flame-graph\n\u2502    \u251c\u2500\u2500   \u2514\u2500\u2500 *.py                           //Flame graph related function script (internal use, users do not need to pay attention)\n\u2502    \u251c\u2500\u2500 startup.sh                           //startup script\n\u2502    \u251c\u2500\u2500 setclasspath.sh                      //Scripts related to java environment variables (internal use, users do not need to pay attention)\n\u2502    \u251c\u2500\u2500 shutdown.sh                          //stop script\n\u2502    \u251c\u2500\u2500 yaml.sh                              //Internally uses a script that parses yaml parameters (for internal use, users don't need to pay attention)\n\u251c\u2500\u2500 conf\n\u2502    \u251c\u2500\u2500 application.yaml                     //Project configuration file (be careful not to change the name)\n\u2502    \u251c\u2500\u2500 flink-application.template           //flink configuration template (for internal use, users don't need to pay attention)\n\u2502    \u251c\u2500\u2500 logback-spring.xml                   //logback\n\u2502    \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 lib\n\u2502    \u2514\u2500\u2500 *.jar                                //Project jar package\n\u251c\u2500\u2500 plugins\n\u2502    \u251c\u2500\u2500 streampark-jvm-profiler-1.0.0.jar       //jvm-profiler, flame graph related functions (internal use, users do not need to pay attention)\n\u2502    \u2514\u2500\u2500 streampark-flink-sqlclient-1.0.0.jar    //Flink SQl submit related functions (for internal use, users do not need to pay attention)\n\u251c\u2500\u2500 script\n\u2502     \u251c\u2500\u2500 schema                               // Complete ddl build table sql\n\u2502     \u251c\u2500\u2500 data                               // The data sql of tables\n\u2502     \u251c\u2500\u2500 upgrade                             // The sql of the upgrade part of each version (only the sql changes from the previous version to this version are recorded)\n\u251c\u2500\u2500 logs                                      //program log directory\n\n\u251c\u2500\u2500 temp                                      //Temporary path used internally, do not delete\n")),(0,r.kt)("h5",{id:"initialize-table-structure"},"Initialize table structure"),(0,r.kt)("p",null,"In the installation process of versions before 1.2.1, there is no need to manually initialize data, just set the database information, and some column operations such as table creation and data initialization will be automatically completed. Versions after 1.2.1 (included) are not included. Automatic table creation and upgrade requires the user to manually execute ddl for initialization. The ddl description is as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-textmate"},"\u251c\u2500\u2500 script\n\u2502     \u251c\u2500\u2500 schema                               // Complete ddl build table sql\n\u2502     \u251c\u2500\u2500 data                               // The data sql of tables\n\u2502     \u251c\u2500\u2500 upgrade                             // The sql of the upgrade part of each version (only the sql changes from the previous version to this version are recorded)\n")),(0,r.kt)("p",null,"Execute the sql files that in ",(0,r.kt)("inlineCode",{parentName:"p"},"schema")," folder to initialize the table structure"),(0,r.kt)("h5",{id:"initialize-table-data"},"Initialize table data"),(0,r.kt)("p",null,"In the installation process of versions before 1.2.1, there is no need to manually initialize data, just set the database information, and some column operations such as table creation and data initialization will be automatically completed. Versions after 1.2.1 (included) are not included. Automatic table creation and upgrade requires the user to manually execute ddl for initialization. The ddl description is as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-textmate"},"\u251c\u2500\u2500 script\n\u2502     \u251c\u2500\u2500 schema                               // Complete ddl build table sql\n\u2502     \u251c\u2500\u2500 data                               // The data sql of tables\n\u2502     \u251c\u2500\u2500 upgrade                             // The sql of the upgrade part of each version (only the sql changes from the previous version to this version are recorded)\n")),(0,r.kt)("p",null,"Execute the sql files that in ",(0,r.kt)("inlineCode",{parentName:"p"},"data")," folder to initialize the table data"),(0,r.kt)("h5",{id:"modify-the-configuration"},"Modify the configuration"),(0,r.kt)("p",null,"The installation and unpacking have been completed, and the next step is to prepare the data-related work"),(0,r.kt)("h6",{id:"create-a-new-database-streampark"},"Create a new database ",(0,r.kt)("inlineCode",{parentName:"h6"},"streampark")),(0,r.kt)("p",null,"Make sure to create a new database ",(0,r.kt)("inlineCode",{parentName:"p"},"streampark")," in mysql that the deployment machine can connect to"),(0,r.kt)("h6",{id:"modify-connection-information"},"Modify connection information"),(0,r.kt)("p",null,"Go to ",(0,r.kt)("inlineCode",{parentName:"p"},"conf"),", modify ",(0,r.kt)("inlineCode",{parentName:"p"},"conf/application.yml"),", find the spring item, find the profiles.active configuration, and modify it to the corresponding information, as follows"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"spring:\n  profiles.active: mysql #[h2,pgsql,mysql]\n  application.name: StreamPark\n  devtools.restart.enabled: false\n  mvc.pathmatch.matching-strategy: ant_path_matcher\n  servlet:\n    multipart:\n      enabled: true\n      max-file-size: 500MB\n      max-request-size: 500MB\n  aop.proxy-target-class: true\n  messages.encoding: utf-8\n  jackson:\n    date-format: yyyy-MM-dd HH:mm:ss\n    time-zone: GMT+8\n  main:\n    allow-circular-references: true\n    banner-mode: off\n")),(0,r.kt)("p",null,"After modify ",(0,r.kt)("inlineCode",{parentName:"p"},"conf/application.yml"),", then modify the ",(0,r.kt)("inlineCode",{parentName:"p"},"config/application-mysql.yml")," to change the config information of database as follows:"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Tips: Because of license incompatibility between Apache project and mysql jdbc driver, so you should download mysql jdbc driver by yourself and put it in $STREAMPARK_HOME/lib")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"spring:\n  datasource:\n    username: root\n    password: xxxx\n    driver-class-name: com.mysql.cj.jdbc.Driver\n    url: jdbc:mysql://localhost:3306/streampark?useSSL=false&useUnicode=true&characterEncoding=UTF-8&allowPublicKeyRetrieval=false&useJDBCCompliantTimezoneShift=true&useLegacyDatetimeCode=false&serverTimezone=GMT%2B8\n")),(0,r.kt)("h6",{id:"modify-workspace"},"Modify workspace"),(0,r.kt)("p",null,"Go to ",(0,r.kt)("inlineCode",{parentName:"p"},"conf"),", modify ",(0,r.kt)("inlineCode",{parentName:"p"},"conf/application.yml"),", find the item streampark, find the workspace configuration, and change it to a directory that the user has permission to."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"streampark:\n  # HADOOP_USER_NAME If it is on yarn mode ( yarn-prejob | yarn-application | yarn-session), you need to configure hadoop-user-name\n  hadoop-user-name: hdfs\n  # Local workspace, used to store project source code, build directory, etc.\n  workspace:\n    local: /opt/streampark_workspace # A local workspace directory (very important), users can change the directory by themselves, it is recommended to put it in other places separately to store the project source code, the built directory, etc.\n    remote: hdfs:///streampark   # support hdfs:///streampark/ \u3001 /streampark \u3001hdfs://host:ip/streampark/\n")),(0,r.kt)("h5",{id:"start-the-backend"},"Start the backend"),(0,r.kt)("p",null,"Enter ",(0,r.kt)("inlineCode",{parentName:"p"},"bin")," and directly execute startup.sh to start the project. The default port is ",(0,r.kt)("strong",{parentName:"p"},"10000"),", if there is no accident, it will start successfully"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd streampark-console-service-1.0.0/bin\nbash startup.sh\n")),(0,r.kt)("p",null,"Relevant logs will be output to ",(0,r.kt)("strong",{parentName:"p"},"streampark-console-service-1.0.0/logs/streampark.out")),(0,r.kt)("admonition",{title:"hint",type:"info"},(0,r.kt)("p",{parentName:"admonition"},"Front-end and back-end mixed packaging mode, only start the back-end service to complete all the deployment, open the browser and enter ",(0,r.kt)("strong",{parentName:"p"},"http://$host:10000")," to log in")),(0,r.kt)("h3",{id:"deploy-frontend"},"Deploy frontend"),(0,r.kt)("h5",{id:"environmental-preparation"},"Environmental preparation"),(0,r.kt)("p",null,"Install nodejs and pm2 globally"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"yum install -y nodejs\nnpm install -g pm2\n")),(0,r.kt)("h5",{id:"release"},"Release"),(0,r.kt)("h6",{id:"1-copy-the-dist-to-the-deployment-server"},"1. Copy the dist to the deployment server"),(0,r.kt)("p",null,"Copy the entire directory of streampark-console-webapp/dist to the deployment directory of the server, such as: ",(0,r.kt)("inlineCode",{parentName:"p"},"/home/www/streampark"),", the copied directory level is /home/www/streampark/dist"),(0,r.kt)("h6",{id:"2-copy-the-streamparkjs-file-to-the-project-deployment-directory"},"2. Copy the streampark.js file to the project deployment directory"),(0,r.kt)("p",null,"Copy streampark/streampark-console/streampark-console-webapp/streampark.js to ",(0,r.kt)("inlineCode",{parentName:"p"},"/home/www/streampark")),(0,r.kt)("h6",{id:"3-modify-the-service-port"},"3. Modify the service port"),(0,r.kt)("p",null,"Users can specify the port address of the front-end service by themselves, modify the /home/www/streampark/streampark.js file, and find ",(0,r.kt)("inlineCode",{parentName:"p"},"serverPort")," to modify, the default is as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"  const serverPort = 1000\n")),(0,r.kt)("ol",{start:4},(0,r.kt)("li",{parentName:"ol"},"Start the service")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"   pm2 start streampark.js\n")),(0,r.kt)("p",null,"For more information about pm2, please refer to ",(0,r.kt)("a",{parentName:"p",href:"https://pm2.keymetrics.io/"},"Official Website")),(0,r.kt)("h3",{id:"log-in-system"},"log in system"),(0,r.kt)("p",null,"After the above steps, even if the deployment is completed, you can directly log in to the system"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"StreamPark Login",src:a(5053).Z,width:"1000",height:"648"})),(0,r.kt)("admonition",{title:"hint",type:"tip"},(0,r.kt)("p",{parentName:"admonition"},"Default password: ",(0,r.kt)("strong",null," admin / streampark "))),(0,r.kt)("h2",{id:"system-configuration"},"System Configuration"),(0,r.kt)("p",null,"After entering the system, the first thing to do is to modify the system configuration. Under the menu/StreamPark/Setting, the operation interface is as follows:"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"StreamPark Settings",src:a(203).Z,width:"2630",height:"2370"})),(0,r.kt)("p",null,"The main configuration items are divided into the following categories"),(0,r.kt)("div",{class:"counter"},(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Flink Home"),(0,r.kt)("li",{parentName:"ul"},"Maven Home"),(0,r.kt)("li",{parentName:"ul"},"StreamPark Env"),(0,r.kt)("li",{parentName:"ul"},"Email"))),(0,r.kt)("h3",{id:"flink-home"},"Flink Home"),(0,r.kt)("p",null,"The global Flink Home is configured here. This is the only place in the system to specify the Flink environment, which will apply to all jobs."),(0,r.kt)("admonition",{title:"hint",type:"info"},(0,r.kt)("p",{parentName:"admonition"},"Special Note: The minimum supported Flink version is 1.12.0, and later versions are supported")),(0,r.kt)("h3",{id:"maven-home"},"Maven Home"),(0,r.kt)("p",null,"Specify maven Home, currently not supported, the next version will be implemented"),(0,r.kt)("h3",{id:"streampark-env"},"StreamPark Env"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"StreamPark Webapp address\nThe web url access address of the StreamPark Console is configured here. The main flame graph function will be used. The specific task will send http requests to the system through the url exposed here for collection and display."),(0,r.kt)("li",{parentName:"ul"},"StreamPark Console Workspace\nThe workspace of the configuration system is used to store the source code of the project, the compiled project, etc. (this configuration is the configuration item in the version before 1.2.0)")),(0,r.kt)("h3",{id:"email"},"Email"),(0,r.kt)("p",null,"The configuration related to Alert Email is to configure the information of the sender's email. For the specific configuration, please refer to the relevant mailbox information and documents for configuration."))}d.isMDXComponent=!0},640:(e,t,a)=>{"use strict";var n=a(1742),r={"text/plain":"Text","text/html":"Url",default:"Text"};e.exports=function(e,t){var a,o,l,i,s,c,m=!1;t||(t={}),a=t.debug||!1;try{if(l=n(),i=document.createRange(),s=document.getSelection(),(c=document.createElement("span")).textContent=e,c.ariaHidden="true",c.style.all="unset",c.style.position="fixed",c.style.top=0,c.style.clip="rect(0, 0, 0, 0)",c.style.whiteSpace="pre",c.style.webkitUserSelect="text",c.style.MozUserSelect="text",c.style.msUserSelect="text",c.style.userSelect="text",c.addEventListener("copy",(function(n){if(n.stopPropagation(),t.format)if(n.preventDefault(),void 0===n.clipboardData){a&&console.warn("unable to use e.clipboardData"),a&&console.warn("trying IE specific stuff"),window.clipboardData.clearData();var o=r[t.format]||r.default;window.clipboardData.setData(o,e)}else n.clipboardData.clearData(),n.clipboardData.setData(t.format,e);t.onCopy&&(n.preventDefault(),t.onCopy(n.clipboardData))})),document.body.appendChild(c),i.selectNodeContents(c),s.addRange(i),!document.execCommand("copy"))throw new Error("copy command was unsuccessful");m=!0}catch(p){a&&console.error("unable to copy using execCommand: ",p),a&&console.warn("trying IE specific stuff");try{window.clipboardData.setData(t.format||"text",e),t.onCopy&&t.onCopy(window.clipboardData),m=!0}catch(p){a&&console.error("unable to copy using clipboardData: ",p),a&&console.error("falling back to prompt"),o=function(e){var t=(/mac os x/i.test(navigator.userAgent)?"\u2318":"Ctrl")+"+C";return e.replace(/#{\s*key\s*}/g,t)}("message"in t?t.message:"Copy to clipboard: #{key}, Enter"),window.prompt(o,e)}}finally{s&&("function"==typeof s.removeRange?s.removeRange(i):s.removeAllRanges()),c&&document.body.removeChild(c),l()}return m}},4300:(e,t,a)=>{"use strict";function n(e){return n="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&"function"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e},n(e)}Object.defineProperty(t,"__esModule",{value:!0}),t.CopyToClipboard=void 0;var r=i(a(7294)),o=i(a(640)),l=["text","onCopy","options","children"];function i(e){return e&&e.__esModule?e:{default:e}}function s(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function c(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?s(Object(a),!0).forEach((function(t){g(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):s(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function m(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}function p(e,t){if(!(e instanceof t))throw new TypeError("Cannot call a class as a function")}function d(e,t){for(var a=0;a<t.length;a++){var n=t[a];n.enumerable=n.enumerable||!1,n.configurable=!0,"value"in n&&(n.writable=!0),Object.defineProperty(e,n.key,n)}}function u(e,t){return u=Object.setPrototypeOf||function(e,t){return e.__proto__=t,e},u(e,t)}function h(e){var t=function(){if("undefined"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Reflect.construct(Boolean,[],(function(){}))),!0}catch(e){return!1}}();return function(){var a,n=k(e);if(t){var r=k(this).constructor;a=Reflect.construct(n,arguments,r)}else a=n.apply(this,arguments);return f(this,a)}}function f(e,t){if(t&&("object"===n(t)||"function"==typeof t))return t;if(void 0!==t)throw new TypeError("Derived constructors may only return object or undefined");return y(e)}function y(e){if(void 0===e)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return e}function k(e){return k=Object.setPrototypeOf?Object.getPrototypeOf:function(e){return e.__proto__||Object.getPrototypeOf(e)},k(e)}function g(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}var b=function(e){!function(e,t){if("function"!=typeof t&&null!==t)throw new TypeError("Super expression must either be null or a function");e.prototype=Object.create(t&&t.prototype,{constructor:{value:e,writable:!0,configurable:!0}}),Object.defineProperty(e,"prototype",{writable:!1}),t&&u(e,t)}(s,e);var t,a,n,i=h(s);function s(){var e;p(this,s);for(var t=arguments.length,a=new Array(t),n=0;n<t;n++)a[n]=arguments[n];return g(y(e=i.call.apply(i,[this].concat(a))),"onClick",(function(t){var a=e.props,n=a.text,l=a.onCopy,i=a.children,s=a.options,c=r.default.Children.only(i),m=(0,o.default)(n,s);l&&l(n,m),c&&c.props&&"function"==typeof c.props.onClick&&c.props.onClick(t)})),e}return t=s,(a=[{key:"render",value:function(){var e=this.props,t=(e.text,e.onCopy,e.options,e.children),a=m(e,l),n=r.default.Children.only(t);return r.default.cloneElement(n,c(c({},a),{},{onClick:this.onClick}))}}])&&d(t.prototype,a),n&&d(t,n),Object.defineProperty(t,"prototype",{writable:!1}),s}(r.default.PureComponent);t.CopyToClipboard=b,g(b,"defaultProps",{onCopy:void 0,options:void 0})},4855:(e,t,a)=>{"use strict";var n=a(4300).CopyToClipboard;n.CopyToClipboard=n,e.exports=n},1742:e=>{e.exports=function(){var e=document.getSelection();if(!e.rangeCount)return function(){};for(var t=document.activeElement,a=[],n=0;n<e.rangeCount;n++)a.push(e.getRangeAt(n));switch(t.tagName.toUpperCase()){case"INPUT":case"TEXTAREA":t.blur();break;default:t=null}return e.removeAllRanges(),function(){"Caret"===e.type&&e.removeAllRanges(),e.rangeCount||a.forEach((function(t){e.addRange(t)})),t&&t.focus()}}},9844:(e,t,a)=>{"use strict";a.d(t,{Z:()=>n});const n=a.p+"assets/images/streampark_archite-8cf7c6f5a116c753f8d9bb546eef06fa.png"},5053:(e,t,a)=>{"use strict";a.d(t,{Z:()=>n});const n=a.p+"assets/images/streampark_login-f3c95ebc1a39382f3da760bc8b576d15.jpeg"},203:(e,t,a)=>{"use strict";a.d(t,{Z:()=>n});const n=a.p+"assets/images/streampark_settings-5927b74e037066cd510550e5a0a873d2.png"}}]);