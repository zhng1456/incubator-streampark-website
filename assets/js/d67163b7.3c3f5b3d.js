"use strict";(self.webpackChunkapache_streampark_website=self.webpackChunkapache_streampark_website||[]).push([[3307],{3905:(e,a,t)=>{t.d(a,{Zo:()=>c,kt:()=>d});var n=t(7294);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function i(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function o(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?i(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=n.createContext({}),p=function(e){var a=n.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):o(o({},a),e)),t},c=function(e){var a=p(e.components);return n.createElement(l.Provider,{value:a},e.children)},m={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},k=n.forwardRef((function(e,a){var t=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),k=p(t),d=r,f=k["".concat(l,".").concat(d)]||k[d]||m[d]||i;return t?n.createElement(f,o(o({ref:a},c),{},{components:t})):n.createElement(f,o({ref:a},c))}));function d(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var i=t.length,o=new Array(i);o[0]=k;var s={};for(var l in a)hasOwnProperty.call(a,l)&&(s[l]=a[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var p=2;p<i;p++)o[p]=t[p];return n.createElement.apply(null,o)}return n.createElement.apply(null,t)}k.displayName="MDXCreateElement"},8949:(e,a,t)=>{t.d(a,{Z:()=>o});var n=t(7294),r=t(6010);const i="tabItem_Ymn6";function o(e){let{children:a,hidden:t,className:o}=e;return n.createElement("div",{role:"tabpanel",className:(0,r.Z)(i,o),hidden:t},a)}},5488:(e,a,t)=>{t.d(a,{Z:()=>d});var n=t(7462),r=t(7294),i=t(6010),o=t(2389),s=t(7392),l=t(7094),p=t(2466);const c="tabList__CuJ",m="tabItem_LNqP";function k(e){var a;const{lazy:t,block:o,defaultValue:k,values:d,groupId:f,className:u}=e,g=r.Children.map(e.children,(e=>{if((0,r.isValidElement)(e)&&"value"in e.props)return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})),h=d??g.map((e=>{let{props:{value:a,label:t,attributes:n}}=e;return{value:a,label:t,attributes:n}})),v=(0,s.l)(h,((e,a)=>e.value===a.value));if(v.length>0)throw new Error(`Docusaurus error: Duplicate values "${v.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`);const S=null===k?k:k??(null==(a=g.find((e=>e.props.default)))?void 0:a.props.value)??g[0].props.value;if(null!==S&&!h.some((e=>e.value===S)))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${S}" but none of its children has the corresponding value. Available values are: ${h.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);const{tabGroupChoices:b,setTabGroupChoices:N}=(0,l.U)(),[y,C]=(0,r.useState)(S),w=[],{blockElementScrollPositionUntilNextRender:T}=(0,p.o5)();if(null!=f){const e=b[f];null!=e&&e!==y&&h.some((a=>a.value===e))&&C(e)}const x=e=>{const a=e.currentTarget,t=w.indexOf(a),n=h[t].value;n!==y&&(T(a),C(n),null!=f&&N(f,String(n)))},K=e=>{var a;let t=null;switch(e.key){case"ArrowRight":{const a=w.indexOf(e.currentTarget)+1;t=w[a]??w[0];break}case"ArrowLeft":{const a=w.indexOf(e.currentTarget)-1;t=w[a]??w[w.length-1];break}}null==(a=t)||a.focus()};return r.createElement("div",{className:(0,i.Z)("tabs-container",c)},r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.Z)("tabs",{"tabs--block":o},u)},h.map((e=>{let{value:a,label:t,attributes:o}=e;return r.createElement("li",(0,n.Z)({role:"tab",tabIndex:y===a?0:-1,"aria-selected":y===a,key:a,ref:e=>w.push(e),onKeyDown:K,onFocus:x,onClick:x},o,{className:(0,i.Z)("tabs__item",m,null==o?void 0:o.className,{"tabs__item--active":y===a})}),t??a)}))),t?(0,r.cloneElement)(g.filter((e=>e.props.value===y))[0],{className:"margin-top--md"}):r.createElement("div",{className:"margin-top--md"},g.map(((e,a)=>(0,r.cloneElement)(e,{key:a,hidden:e.props.value!==y})))))}function d(e){const a=(0,o.Z)();return r.createElement(k,(0,n.Z)({key:String(a)},e))}},5432:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>c,contentTitle:()=>l,default:()=>d,frontMatter:()=>s,metadata:()=>p,toc:()=>m});var n=t(7462),r=(t(7294),t(3905)),i=t(5488),o=t(8949);const s={id:"Kafka-Connector",title:"Apache Kafka Connector",sidebar_position:1},l=void 0,p={unversionedId:"connector/Kafka-Connector",id:"connector/Kafka-Connector",title:"Apache Kafka Connector",description:"Flink officially provides a connector to Apache Kafka connector for reading from or writing to a Kafka topic, providing exactly once processing semantics",source:"@site/docs/connector/1-kafka.md",sourceDirName:"connector",slug:"/connector/Kafka-Connector",permalink:"/docs/connector/Kafka-Connector",draft:!1,editUrl:"https://github.com/apache/incubator-streampark-website/edit/dev/docs/connector/1-kafka.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{id:"Kafka-Connector",title:"Apache Kafka Connector",sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Alert configuration",permalink:"/docs/development/alert-conf"},next:{title:"JDBC Connector",permalink:"/docs/connector/Jdbc-Connector"}},c={},m=[{value:"Dependencies",id:"dependencies",level:2},{value:"Kafka Source (Consumer)",id:"kafka-source-consumer",level:2},{value:"example",id:"example",level:3},{value:"Advanced configuration parameters",id:"advanced-configuration-parameters",level:3},{value:"Consume multiple Kafka instances",id:"consume-multiple-kafka-instances",level:3},{value:"Consume multiple topics",id:"consume-multiple-topics",level:3},{value:"Topic dynamic discovery",id:"topic-dynamic-discovery",level:3},{value:"Consume from the specified offset",id:"consume-from-the-specified-offset",level:3},{value:"Consume from the specified offset",id:"consume-from-the-specified-offset-1",level:3},{value:"Specific deserializer",id:"specific-deserializer",level:3},{value:"Return record Kafka Record",id:"return-record-kafka-record",level:3},{value:"Specific strategy",id:"specific-strategy",level:3},{value:"Kafka Sink (Producer)",id:"kafka-sink-producer",level:2},{value:"Fault Tolerance and Semantics",id:"fault-tolerance-and-semantics",level:3},{value:"Multiple instance kafka specifies alias",id:"multiple-instance-kafka-specifies-alias",level:3},{value:"Specific SerializationSchema",id:"specific-serializationschema",level:3},{value:"Specific SerializationSchema",id:"specific-serializationschema-1",level:3},{value:"specific partitioner",id:"specific-partitioner",level:3}],k={toc:m};function d(e){let{components:a,...s}=e;return(0,r.kt)("wrapper",(0,n.Z)({},k,s,{components:a,mdxType:"MDXLayout"}),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/kafka.html"},"Flink officially")," provides a connector to ",(0,r.kt)("a",{parentName:"p",href:"https://kafka.apache.org"},"Apache Kafka")," connector for reading from or writing to a Kafka topic, providing ",(0,r.kt)("strong",{parentName:"p"},"exactly once")," processing semantics"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"KafkaSource")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"KafkaSink")," in ",(0,r.kt)("inlineCode",{parentName:"p"},"StreamPark")," are further encapsulated based on ",(0,r.kt)("inlineCode",{parentName:"p"},"kafka connector")," from the official website, simplifying the development steps, making it easier to read and write data"),(0,r.kt)("h2",{id:"dependencies"},"Dependencies"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/kafka.html"},"Apache Flink")," integrates with the generic Kafka connector, which tries to keep up with the latest version of the Kafka client. The version of the Kafka client used by this connector may change between Flink versions. The current Kafka client is backward compatible with Kafka broker version 0.10.0 or later. For more details on Kafka compatibility, please refer to the ",(0,r.kt)("a",{parentName:"p",href:"https://kafka.apache.org/protocol.html#protocol_compatibility"},"Apache Kafka")," official documentation."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"    <dependency>\n        <groupId>org.apache.streampark/groupId>\n        <artifactId>streampark-flink-core</artifactId>\n        <version>${project.version}</version>\n    </dependency>\n\n    <dependency>\n        <groupId>org.apache.flink</groupId>\n        <artifactId>flink-connector-kafka_2.11</artifactId>\n        <version>1.12.0</version>\n    </dependency>\n")),(0,r.kt)("p",null,"In the development phase, the following dependencies are also necessary"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"    <dependency>\n        <groupId>org.apache.flink</groupId>\n        <artifactId>flink-scala_${scala.binary.version}</artifactId>\n        <version>${flink.version}</version>\n        <scope>provided</scope>\n    </dependency>\n\n    <dependency>\n        <groupId>org.apache.flink</groupId>\n        <artifactId>flink-clients_${scala.binary.version}</artifactId>\n        <version>${flink.version}</version>\n        <scope>provided</scope>\n    </dependency>\n\n    <dependency>\n        <groupId>org.apache.flink</groupId>\n        <artifactId>flink-streaming-scala_${scala.binary.version}</artifactId>\n        <version>${flink.version}</version>\n        <scope>provided</scope>\n    </dependency>\n")),(0,r.kt)("h2",{id:"kafka-source-consumer"},"Kafka Source (Consumer)"),(0,r.kt)("p",null,"First, we introduce the standard kafka consumer approach based on the official website, the following code is taken from the ",(0,r.kt)("a",{parentName:"p",href:"https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/kafka.html#kafka-consumer"},"official website documentation")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'val properties = new Properties()\nproperties.setProperty("bootstrap.servers", "localhost:9092")\nproperties.setProperty("group.id", "test")\nval stream = env.addSource(new FlinkKafkaConsumer[String]("topic", new SimpleStringSchema(), properties))\n\n')),(0,r.kt)("p",null,"You can see a series of kafka connection information defined, this way the parameters are hard-coded, very insensitive, let's see how to use ",(0,r.kt)("inlineCode",{parentName:"p"},"StreamPark")," to access ",(0,r.kt)("inlineCode",{parentName:"p"},"kafka")," data, we just define the configuration file in the rule format and then write the code"),(0,r.kt)("h3",{id:"example"},"example"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"kafka.source:\n  bootstrap.servers: kfk1:9092,kfk2:9092,kfk3:9092\n  topic: test_user\n  group.id: user_01\n  auto.offset.reset: earliest\n  enable.auto.commit: true\n")),(0,r.kt)("admonition",{title:"Cautions",type:"info"},(0,r.kt)("p",{parentName:"admonition"},"The prefix ",(0,r.kt)("inlineCode",{parentName:"p"},"kafka.source")," is fixed, and the parameters related to kafka properties must comply with the ",(0,r.kt)("a",{parentName:"p",href:"http://kafka.apache.org"},"kafka official website")," specification for setting the parameter key")),(0,r.kt)(i.Z,{mdxType:"Tabs"},(0,r.kt)(o.Z,{value:"scala",label:"Scala",default:!0,mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},"package org.apache.streampark.flink.quickstart\n\nimport org.apache.streampark.flink.core.scala.FlinkStreaming\nimport org.apache.streampark.flink.core.scala.sink.JdbcSink\nimport org.apache.streampark.flink.core.scala.source.KafkaSource\nimport org.apache.flink.api.scala._\n\nobject kafkaSourceApp extends FlinkStreaming {\n\n    override def handle(): Unit = {\n        val source = KafkaSource().getDataStream[String]()\n        print(source)\n    }\n\n}\n\n"))),(0,r.kt)(o.Z,{value:"Java",label:"Java",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},"import org.apache.streampark.flink.core.java.function.StreamEnvConfigFunction;\nimport org.apache.streampark.flink.core.java.source.KafkaSource;\nimport org.apache.streampark.flink.core.scala.StreamingContext;\nimport org.apache.streampark.flink.core.scala.source.KafkaRecord;\nimport org.apache.streampark.flink.core.scala.util.StreamEnvConfig;\nimport org.apache.flink.api.common.functions.MapFunction;\nimport org.apache.flink.streaming.api.datastream.DataStream;\n\npublic class KafkaSimpleJavaApp {\n\n    public static void main(String[] args) {\n        StreamEnvConfig envConfig = new StreamEnvConfig(args, null);\n        StreamingContext context = new StreamingContext(envConfig);\n        DataStream<String> source = new KafkaSource<String>(context)\n                .getDataStream()\n                .map((MapFunction<KafkaRecord<String>, String>) KafkaRecord::value);\n\n        source.print();\n\n        context.start();\n    }\n}\n\n")))),(0,r.kt)("h3",{id:"advanced-configuration-parameters"},"Advanced configuration parameters"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"KafkaSource")," is based on the Flink Kafka Connector construct a simpler kafka reading class, the constructor needs to pass ",(0,r.kt)("inlineCode",{parentName:"p"},"StreamingContext"),", when the program starts to pass the configuration file can be, framework will automatically parse the configuration file, when ",(0,r.kt)("inlineCode",{parentName:"p"},"new KafkaSource")," it will automatically get the relevant information from the configuration file, initialize and return a Kafka Consumer, in this case, only configuration one topic, so in the consumption of the time without specifying the topic directly by default to get this topic to consume, this is the simple example, more complex rules and read operations through the ",(0,r.kt)("inlineCode",{parentName:"p"},". getDataStream()")," pass parameters in the method to achieve\nLet's look at the signature of the ",(0,r.kt)("inlineCode",{parentName:"p"},"getDataStream")," method"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'def getDataStream[T: TypeInformation](topic: java.io.Serializable = null,\n    alias: String = "",\n    deserializer: KafkaDeserializationSchema[T],\n    strategy: WatermarkStrategy[KafkaRecord[T]] = null\n): DataStream[KafkaRecord[T]]\n')),(0,r.kt)("p",null,"The specific description of the parameters are as follows"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:"left"},"Parameter Name"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Parameter Type"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Description"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Default"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("inlineCode",{parentName:"td"},"topic")),(0,r.kt)("td",{parentName:"tr",align:"left"},"Serializable"),(0,r.kt)("td",{parentName:"tr",align:"left"},"a topic or group of topics"),(0,r.kt)("td",{parentName:"tr",align:"left"})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("inlineCode",{parentName:"td"},"alias")),(0,r.kt)("td",{parentName:"tr",align:"left"},"String"),(0,r.kt)("td",{parentName:"tr",align:"left"},"distinguish different kafka instances"),(0,r.kt)("td",{parentName:"tr",align:"left"})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("inlineCode",{parentName:"td"},"deserializer")),(0,r.kt)("td",{parentName:"tr",align:"left"},"DeserializationSchema"),(0,r.kt)("td",{parentName:"tr",align:"left"},"deserialize class of the data in the topic"),(0,r.kt)("td",{parentName:"tr",align:"left"},"KafkaStringDeserializationSchema")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("inlineCode",{parentName:"td"},"strategy")),(0,r.kt)("td",{parentName:"tr",align:"left"},"WatermarkStrategy"),(0,r.kt)("td",{parentName:"tr",align:"left"},"watermark generation strategy"),(0,r.kt)("td",{parentName:"tr",align:"left"})))),(0,r.kt)("p",null,"Let's take a look at more usage and configuration methods"),(0,r.kt)("div",{class:"counter"},(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Consume multiple Kafka instances"),(0,r.kt)("li",{parentName:"ul"},"Consume multiple topics"),(0,r.kt)("li",{parentName:"ul"},"Topic dynamic discovery"),(0,r.kt)("li",{parentName:"ul"},"Consume from the specified offset"),(0,r.kt)("li",{parentName:"ul"},"Specific KafkaDeserializationSchema"),(0,r.kt)("li",{parentName:"ul"},"Specific WatermarkStrategy"))),(0,r.kt)("h3",{id:"consume-multiple-kafka-instances"},"Consume multiple Kafka instances"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"StreamPark")," has taken into account the configuration of kafka of multiple different instances at the beginning of development . How to unify the configuration, and standardize the format? The solution in streampark is this, if we want to consume two different instances of kafka at the same time, the configuration file is defined as follows,\nAs you can see in the ",(0,r.kt)("inlineCode",{parentName:"p"},"kafka.source")," directly under the kafka instance name, here we unified called ",(0,r.kt)("strong",{parentName:"p"},"alias")," , ",(0,r.kt)("strong",{parentName:"p"},"alias")," must be unique, to distinguish between different instances\nIf there is only one kafka instance, then you can not configure ",(0,r.kt)("inlineCode",{parentName:"p"},"alias"),"\nWhen writing the code for consumption, pay attention to the corresponding ",(0,r.kt)("strong",{parentName:"p"},"alias")," can be specified, the configuration and code is as follows"),(0,r.kt)(i.Z,{mdxType:"Tabs"},(0,r.kt)(o.Z,{value:"Setting",label:"Setting",default:!0,mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"kafka.source:\n  kafka1:\n    bootstrap.servers: kfk1:9092,kfk2:9092,kfk3:9092\n    topic: test_user\n    group.id: user_01\n    auto.offset.reset: earliest\n    enable.auto.commit: true\n  kafka2:\n    bootstrap.servers: kfk4:9092,kfk5:9092,kfk6:9092\n    topic: kafka2\n    group.id: kafka2\n    auto.offset.reset: earliest\n    enable.auto.commit: true\n"))),(0,r.kt)(o.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'KafkaSource().getDataStream[String](alias = "kafka1")\n  .uid("kfkSource1")\n  .name("kfkSource1")\n  .print()\n\nKafkaSource().getDataStream[String](alias = "kafka2")\n  .uid("kfkSource2")\n  .name("kfkSource2")\n  .print()\n\n'))),(0,r.kt)(o.Z,{value:"Java",label:"Java",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},'StreamEnvConfig envConfig = new StreamEnvConfig(args, null);\nStreamingContext context = new StreamingContext(envConfig);\n\nDataStream<String> source1 = new KafkaSource<String>(context)\n        .alias("kafka1")\n        .getDataStream()\n        print();\n\nDataStream<String> source2 = new KafkaSource<String>(context)\n        .alias("kafka2")\n        .getDataStream()\n        .print();\n\ncontext.start();\n')),(0,r.kt)("admonition",{title:"Cautions",type:"danger"},(0,r.kt)("p",{parentName:"admonition"},"When writing code in java api, be sure to place the settings of these parameters such as ",(0,r.kt)("inlineCode",{parentName:"p"},"alias")," before calling ",(0,r.kt)("inlineCode",{parentName:"p"},"getDataStream()"))))),(0,r.kt)("h3",{id:"consume-multiple-topics"},"Consume multiple topics"),(0,r.kt)("p",null,"Configure the consumption of multiple topic is also very simple, in the configuration file ",(0,r.kt)("inlineCode",{parentName:"p"},"topic")," can be configured under multiple topic name, separated by ",(0,r.kt)("inlineCode",{parentName:"p"},",")," or space, in the ",(0,r.kt)("inlineCode",{parentName:"p"},"scala")," api, if the consumption of a topic, then directly pass the topic name can be, if you want to consume multiple, pass a ",(0,r.kt)("inlineCode",{parentName:"p"},"List")," can be\n",(0,r.kt)("inlineCode",{parentName:"p"},"java")," api through the ",(0,r.kt)("inlineCode",{parentName:"p"},"topic()")," method to pass in the topic name, which is a variable parameter of type String, can be accepted in one or more ",(0,r.kt)("inlineCode",{parentName:"p"},"topic")," name, configuration and code as follows"),(0,r.kt)(i.Z,{mdxType:"Tabs"},(0,r.kt)(o.Z,{value:"Setting",label:"Setting",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"kafka.source:\n  bootstrap.servers: kfk1:9092,kfk2:9092,kfk3:9092\n  topic: topic1,topic2,topic3...\n  group.id: user_01\n  auto.offset.reset: earliest # (earliest | latest)\n  ...\n"))),(0,r.kt)(o.Z,{value:"Scala",label:"Scala",default:!0,mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'KafkaSource().getDataStream[String](topic = "topic1")\n  .uid("kfkSource1")\n  .name("kfkSource1")\n  .print()\n\nKafkaSource().getDataStream[String](topic = List("topic1","topic2","topic3"))\n.uid("kfkSource1")\n.name("kfkSource1")\n.print()\n\n'))),(0,r.kt)(o.Z,{value:"Java",label:"Java",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},'DataStream<String> source1 = new KafkaSource<String>(context)\n        .topic("topic1")\n        .getDataStream()\n        .print();\n\nDataStream<String> source1 = new KafkaSource<String>(context)\n        .topic("topic1","topic2")\n        .getDataStream()\n        .print();\n\n')))),(0,r.kt)("admonition",{title:"tip",type:"tip"},(0,r.kt)("p",{parentName:"admonition"},(0,r.kt)("inlineCode",{parentName:"p"},"topic")," supports configuring multiple instances of ",(0,r.kt)("inlineCode",{parentName:"p"},"topic"),", each ",(0,r.kt)("inlineCode",{parentName:"p"},"topic")," is directly separated by ",(0,r.kt)("inlineCode",{parentName:"p"},",")," or separated by spaces, if multiple instances are configured under the topic, the specific topic name must be specified when consuming")),(0,r.kt)("h3",{id:"topic-dynamic-discovery"},"Topic dynamic discovery"),(0,r.kt)("p",null,"Regarding kafka's partition dynamics, by default, partition discovery is disabled. To enable it, please set ",(0,r.kt)("inlineCode",{parentName:"p"},"flink.partition-discovery.interval-millis")," to greater than ",(0,r.kt)("inlineCode",{parentName:"p"},"0")," in the provided configuration, which means the interval between partition discovery is in milliseconds\nFor more details, please refer to the ",(0,r.kt)("a",{parentName:"p",href:"https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/kafka.html#partition-discovery"},"official website documentation")),(0,r.kt)("p",null,"Flink Kafka Consumer is also able to discover Topics using regular expressions, please refer to the ",(0,r.kt)("a",{parentName:"p",href:"https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/kafka.html#topic-discovery"},"official website documentation"),"\nA simpler way is provided in ",(0,r.kt)("inlineCode",{parentName:"p"},"StreamPark"),", you need to configure the regular pattern of the matching ",(0,r.kt)("inlineCode",{parentName:"p"},"topic")," instance name in ",(0,r.kt)("inlineCode",{parentName:"p"},"pattern")),(0,r.kt)(i.Z,{mdxType:"Tabs"},(0,r.kt)(o.Z,{value:"Setting",label:"Setting",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"kafka.source:\n  bootstrap.servers: kfk1:9092,kfk2:9092,kfk3:9092\n  pattern: ^topic[1-9]\n  group.id: user_02\n  auto.offset.reset: earliest # (earliest | latest)\n  ...\n"))),(0,r.kt)(o.Z,{value:"Scala",label:"Scala",default:!0,mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'KafkaSource().getDataStream[String](topic = "topic-a")\n.uid("kfkSource1")\n.name("kfkSource1")\n.print()\n'))),(0,r.kt)(o.Z,{value:"Java",label:"Java",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},'StreamEnvConfig envConfig = new StreamEnvConfig(args, null);\nStreamingContext context = new StreamingContext(envConfig);\n\nnew KafkaSource<String>(context)\n        .topic("topic-a")\n        .getDataStream()\n        .print();\n\ncontext.start();\n')))),(0,r.kt)("admonition",{title:"Cautions",type:"danger"},(0,r.kt)("p",{parentName:"admonition"},(0,r.kt)("inlineCode",{parentName:"p"},"topic")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"pattern")," can not be configured at the same time, when configured with ",(0,r.kt)("inlineCode",{parentName:"p"},"pattern")," regular match, you can still specify a definite ",(0,r.kt)("inlineCode",{parentName:"p"},"topic")," name, at this time, will check whether ",(0,r.kt)("inlineCode",{parentName:"p"},"pattern")," matches the current ",(0,r.kt)("inlineCode",{parentName:"p"},"topic"),", if not, will be reported an error")),(0,r.kt)("h3",{id:"consume-from-the-specified-offset"},"Consume from the specified offset"),(0,r.kt)("p",null,"Flink Kafka Consumer allows the starting position of Kafka partitions to be determined by configuration, ",(0,r.kt)("a",{parentName:"p",href:"https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/kafka.html#kafka-consumers-start-position-configuration"},"official website documentation")," The starting position of a Kafka partition is specified as follows"),(0,r.kt)(i.Z,{mdxType:"Tabs"},(0,r.kt)(o.Z,{value:"scala",default:!0,mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},"val env = StreamExecutionEnvironment.getExecutionEnvironment()\nval myConsumer = new FlinkKafkaConsumer[String](...)\nmyConsumer.setStartFromEarliest()\nmyConsumer.setStartFromLatest()\nmyConsumer.setStartFromTimestamp(...)\nmyConsumer.setStartFromGroupOffsets()\n\nval stream = env.addSource(myConsumer)\n...\n"))),(0,r.kt)(o.Z,{value:"Java",label:"Java",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},"final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\nFlinkKafkaConsumer<String> myConsumer = new FlinkKafkaConsumer<>(...);\nmyConsumer.setStartFromEarliest();\nmyConsumer.setStartFromLatest();\nmyConsumer.setStartFromTimestamp(...);\nmyConsumer.setStartFromGroupOffsets();\n\nDataStream<String> stream = env.addSource(myConsumer);\n...\n")))),(0,r.kt)("p",null,"This setting is not recommended in ",(0,r.kt)("inlineCode",{parentName:"p"},"StreamPark"),", a more convenient way is provided by specifying ",(0,r.kt)("inlineCode",{parentName:"p"},"auto.offset.reset")," in the configuration"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"earliest")," consume from earliest record"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"latest")," consume from latest record")),(0,r.kt)("h3",{id:"consume-from-the-specified-offset-1"},"Consume from the specified offset"),(0,r.kt)("p",null,"You can also specify the offset to start consumption for each partition, simply by configuring the ",(0,r.kt)("inlineCode",{parentName:"p"},"start.from")," related information in the following configuration file"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"kafka.source:\n  bootstrap.servers: kfk1:9092,kfk2:9092,kfk3:9092\n  topic: topic1,topic2,topic3...\n  group.id: user_01\n  auto.offset.reset: earliest # (earliest | latest)\n  start.from:\n    timestamp: 1591286400000 # Specify timestamp to take effect for all topics\n    offset: # Specify an offset for the topic's partition\n      topic: topic_abc,topic_123\n      topic_abc: 0:182,1:183,2:182 # Partition 0 starts consumption from 182, partition 1 starts from 183, and partition 2 starts from 182...\n      topic_123: 0:182,1:183,2:182\n  ...\n")),(0,r.kt)("h3",{id:"specific-deserializer"},"Specific deserializer"),(0,r.kt)("p",null,"If you do not specify ",(0,r.kt)("inlineCode",{parentName:"p"},"deserializer"),", the data in the topic will be deserialized by using String by default. At the same time, you can also manually specify ",(0,r.kt)("inlineCode",{parentName:"p"},"deserializer"),". The complete code is as follows"),(0,r.kt)(i.Z,{mdxType:"Tabs"},(0,r.kt)(o.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},"import org.apache.streampark.common.util.JsonUtils\nimport org.apache.streampark.flink.core.scala.FlinkStreaming\nimport org.apache.streampark.flink.core.scala.sink.JdbcSink\nimport org.apache.streampark.flink.core.scala.source.KafkaSource\nimport org.apache.flink.api.common.typeinfo.TypeInformation\nimport org.apache.flink.api.java.typeutils.TypeExtractor.getForClass\nimport org.apache.flink.api.scala._\nimport org.apache.flink.streaming.connectors.kafka.KafkaDeserializationSchema\n\nobject KafkaSourceApp extends FlinkStreaming {\n\n  override def handle(): Unit = {\n        KafkaSource()\n          .getDataStream[String](deserializer = new UserSchema)\n          .map(_.value)\n          .print()\n  }\n\n}\n\nclass UserSchema extends KafkaDeserializationSchema[User] {\n  override def isEndOfStream(nextElement: User): Boolean = false\n  override def getProducedType: TypeInformation[User] = getForClass(classOf[User])\n  override def deserialize(record: ConsumerRecord[Array[Byte], Array[Byte]]): User = {\n    val value = new String(record.value())\n    JsonUtils.read[User](value)\n  }\n}\n\ncase class User(name:String,age:Int,gender:Int,address:String)\n\n"))),(0,r.kt)(o.Z,{value:"Java",label:"Java",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},"import com.fasterxml.jackson.databind.ObjectMapper;\nimport org.apache.streampark.flink.core.java.function.StreamEnvConfigFunction;\nimport org.apache.streampark.flink.core.java.source.KafkaSource;\nimport org.apache.streampark.flink.core.scala.StreamingContext;\nimport org.apache.streampark.flink.core.scala.source.KafkaRecord;\nimport org.apache.streampark.flink.core.scala.util.StreamEnvConfig;\nimport org.apache.flink.api.common.functions.MapFunction;\nimport org.apache.flink.api.common.typeinfo.TypeInformation;\nimport org.apache.flink.streaming.connectors.kafka.KafkaDeserializationSchema;\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\n\nimport java.io.Serializable;\n\nimport static org.apache.flink.api.java.typeutils.TypeExtractor.getForClass;\n\npublic class KafkaSourceJavaApp {\n\n    public static void main(String[] args) {\n        StreamEnvConfig envConfig = new StreamEnvConfig(args, null);\n        StreamingContext context = new StreamingContext(envConfig);\n        new KafkaSource<JavaUser>(context)\n                .deserializer(new JavaUserSchema())\n                .getDataStream()\n                .map((MapFunction<KafkaRecord<JavaUser>, JavaUser>) KafkaRecord::value)\n                .print();\n\n        context.start();\n    }\n\n}\n\nclass JavaUserSchema implements KafkaDeserializationSchema<JavaUser> {\n    private ObjectMapper mapper = new ObjectMapper();\n    @Override public boolean isEndOfStream(JavaUser nextElement) return false;\n    @Override public TypeInformation<JavaUser> getProducedType() return getForClass(JavaUser.class);\n    @Override public JavaUser deserialize(ConsumerRecord<byte[], byte[]> record) throws Exception {\n        String value = new String(record.value());\n        return mapper.readValue(value, JavaUser.class);\n    }\n}\n\nclass JavaUser implements Serializable {\n    String name;\n    Integer age;\n    Integer gender;\n    String address;\n}\n")))),(0,r.kt)("h3",{id:"return-record-kafka-record"},"Return record Kafka Record"),(0,r.kt)("p",null,"The returned object is wrapped in a ",(0,r.kt)("inlineCode",{parentName:"p"},"KafkaRecord"),", which has the current ",(0,r.kt)("inlineCode",{parentName:"p"},"offset"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"partition"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"timestamp")," and many other useful information for developers to use, where ",(0,r.kt)("inlineCode",{parentName:"p"},"value")," is the target object returned, as shown below:"),(0,r.kt)("p",null,(0,r.kt)("img",{src:t(7198).Z,width:"1936",height:"1196"})),(0,r.kt)("h3",{id:"specific-strategy"},"Specific strategy"),(0,r.kt)("p",null,"In many case, the timestamp of the record is embedded (explicitly or implicitly) in the record itself. In addition, users may want to specify in a custom way, for example a special record in a ",(0,r.kt)("inlineCode",{parentName:"p"},"Kafka")," stream containing a ",(0,r.kt)("inlineCode",{parentName:"p"},"watermark")," of the current event time. For these cases, ",(0,r.kt)("inlineCode",{parentName:"p"},"Flink Kafka Consumer")," is allowed to specify ",(0,r.kt)("inlineCode",{parentName:"p"},"AssignerWithPeriodicWatermarks")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"AssignerWithPunctuatedWatermarks"),"."),(0,r.kt)("p",null,"In the ",(0,r.kt)("inlineCode",{parentName:"p"},"StreamPark")," run pass a ",(0,r.kt)("inlineCode",{parentName:"p"},"WatermarkStrategy")," as a parameter to assign a ",(0,r.kt)("inlineCode",{parentName:"p"},"Watermark"),", for example, parse the data in the ",(0,r.kt)("inlineCode",{parentName:"p"},"topic")," as a ",(0,r.kt)("inlineCode",{parentName:"p"},"user")," object, there is an ",(0,r.kt)("inlineCode",{parentName:"p"},"orderTime")," in ",(0,r.kt)("inlineCode",{parentName:"p"},"user")," which is a time type, we use this as a base to assign a ",(0,r.kt)("inlineCode",{parentName:"p"},"Watermark")," to it"),(0,r.kt)(i.Z,{mdxType:"Tabs"},(0,r.kt)(o.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},"import org.apache.streampark.common.util.JsonUtils\nimport org.apache.streampark.flink.core.scala.FlinkStreaming\nimport org.apache.streampark.flink.core.scala.source.{KafkaRecord, KafkaSource}\nimport org.apache.flink.api.common.eventtime.{SerializableTimestampAssigner, WatermarkStrategy}\nimport org.apache.flink.api.common.typeinfo.TypeInformation\nimport org.apache.flink.api.java.typeutils.TypeExtractor.getForClass\nimport org.apache.flink.api.scala._\nimport org.apache.flink.streaming.connectors.kafka.KafkaDeserializationSchema\nimport org.apache.kafka.clients.consumer.ConsumerRecord\n\nimport java.time.Duration\nimport java.util.Date\n\nobject KafkaSourceStrategyApp extends FlinkStreaming {\n\n  override def handle(): Unit = {\n    KafkaSource()\n      .getDataStream[User](\n        deserializer = new UserSchema,\n        strategy = WatermarkStrategy\n          .forBoundedOutOfOrderness[KafkaRecord[User]](Duration.ofMinutes(1))\n          .withTimestampAssigner(new SerializableTimestampAssigner[KafkaRecord[User]] {\n            override def extractTimestamp(element: KafkaRecord[User], recordTimestamp: Long): Long = {\n              element.value.orderTime.getTime\n            }\n          })\n      ).map(_.value)\n      .print()\n  }\n\n}\n\nclass UserSchema extends KafkaDeserializationSchema[User] {\n  override def isEndOfStream(nextElement: User): Boolean = false\n  override def getProducedType: TypeInformation[User] = getForClass(classOf[User])\n  override def deserialize(record: ConsumerRecord[Array[Byte], Array[Byte]]): User = {\n    val value = new String(record.value())\n    JsonUtils.read[User](value)\n  }\n}\n\ncase class User(name: String, age: Int, gender: Int, address: String, orderTime: Date)\n\n"))),(0,r.kt)(o.Z,{value:"Java",label:"Java",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},"import com.fasterxml.jackson.databind.ObjectMapper;\nimport org.apache.streampark.flink.core.java.function.StreamEnvConfigFunction;\nimport org.apache.streampark.flink.core.java.source.KafkaSource;\nimport org.apache.streampark.flink.core.scala.StreamingContext;\nimport org.apache.streampark.flink.core.scala.source.KafkaRecord;\nimport org.apache.streampark.flink.core.scala.util.StreamEnvConfig;\nimport org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;\nimport org.apache.flink.api.common.eventtime.WatermarkStrategy;\nimport org.apache.flink.api.common.functions.MapFunction;\nimport org.apache.flink.api.common.typeinfo.TypeInformation;\nimport org.apache.flink.streaming.connectors.kafka.KafkaDeserializationSchema;\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\n\nimport java.io.Serializable;\nimport java.time.Duration;\nimport java.util.Date;\n\nimport static org.apache.flink.api.java.typeutils.TypeExtractor.getForClass;\n\npublic class KafkaSourceStrategyJavaApp {\n\n    public static void main(String[] args) {\n        StreamEnvConfig envConfig = new StreamEnvConfig(args, null);\n        StreamingContext context = new StreamingContext(envConfig);\n        new KafkaSource<JavaUser>(context)\n                .deserializer(new JavaUserSchema())\n                .strategy(\n                        WatermarkStrategy.<KafkaRecord<JavaUser>>forBoundedOutOfOrderness(Duration.ofMinutes(1))\n                                .withTimestampAssigner(\n                                        (SerializableTimestampAssigner<KafkaRecord<JavaUser>>)\n                                                (element, recordTimestamp) -> element.value().orderTime.getTime()\n                                )\n                )\n                .getDataStream()\n                .map((MapFunction<KafkaRecord<JavaUser>, JavaUser>) KafkaRecord::value)\n                .print();\n\n\n        context.start();\n    }\n\n}\n\nclass JavaUserSchema implements KafkaDeserializationSchema<JavaUser> {\n    private ObjectMapper mapper = new ObjectMapper();\n    @Override public boolean isEndOfStream(JavaUser nextElement) return false;\n    @Override public TypeInformation<JavaUser> getProducedType() return getForClass(JavaUser.class);\n    @Override public JavaUser deserialize(ConsumerRecord<byte[], byte[]> record) throws Exception {\n        String value = new String(record.value());\n        return mapper.readValue(value, JavaUser.class);\n    }\n}\n\nclass JavaUser implements Serializable {\n    String name;\n    Integer age;\n    Integer gender;\n    String address;\n    Date orderTime;\n}\n")))),(0,r.kt)("admonition",{title:"Cautions",type:"info"},(0,r.kt)("p",{parentName:"admonition"},"If the ",(0,r.kt)("inlineCode",{parentName:"p"},"watermark assigner")," relies on messages read from ",(0,r.kt)("inlineCode",{parentName:"p"},"Kafka")," to raise the ",(0,r.kt)("inlineCode",{parentName:"p"},"watermark")," (which is usually the case), then all topics and partitions need to have a continuous stream of messages. Otherwise, ",(0,r.kt)("strong",{parentName:"p"},"the application's ",(0,r.kt)("inlineCode",{parentName:"strong"},"watermark")," will not rise")," and all time-based arithmetic (such as time windows or functions with timers) will not work. A single ",(0,r.kt)("inlineCode",{parentName:"p"},"Kafka")," partition can also cause this reaction. Consider setting the appropriate ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("a",{parentName:"strong",href:"https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/event_timestamps_watermarks.html#dealing-with-idle-sources"},(0,r.kt)("inlineCode",{parentName:"a"},"idelness timeouts")))," to mitigate this problem.")),(0,r.kt)("h2",{id:"kafka-sink-producer"},"Kafka Sink (Producer)"),(0,r.kt)("p",null,"In ",(0,r.kt)("inlineCode",{parentName:"p"},"StreamPark")," the ",(0,r.kt)("inlineCode",{parentName:"p"},"Kafka Producer")," is called ",(0,r.kt)("inlineCode",{parentName:"p"},"KafkaSink"),", which allows messages to be written to one or more ",(0,r.kt)("inlineCode",{parentName:"p"},"Kafka topics"),"."),(0,r.kt)(i.Z,{mdxType:"Tabs"},(0,r.kt)(o.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"}," val source = KafkaSource().getDataStream[String]().map(_.value)\n KafkaSink().sink(source)\n"))),(0,r.kt)(o.Z,{value:"Java",label:"Java",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"}," StreamEnvConfig envConfig = new StreamEnvConfig(args, null);\n StreamingContext context = new StreamingContext(envConfig);\n DataStream<String> source = new KafkaSource<String>(context)\n         .getDataStream()\n         .map((MapFunction<KafkaRecord<String>, String>) KafkaRecord::value);\n\n new KafkaSink<String>(context).sink(source);\n\n context.start();\n")))),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"sink")," is a specific method for writing data, and the list of parameters is as follows"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:"left"},"Parameter Name"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Parameter Type"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Description"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Default"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Required"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("inlineCode",{parentName:"td"},"stream")),(0,r.kt)("td",{parentName:"tr",align:"left"},"DataStream","[T]"),(0,r.kt)("td",{parentName:"tr",align:"left"},"data stream to write"),(0,r.kt)("td",{parentName:"tr",align:"left"}),(0,r.kt)("td",{parentName:"tr",align:"left"},"yes")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("inlineCode",{parentName:"td"},"alias")),(0,r.kt)("td",{parentName:"tr",align:"left"},"String"),(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("inlineCode",{parentName:"td"},"kafka")," instance alias"),(0,r.kt)("td",{parentName:"tr",align:"left"}),(0,r.kt)("td",{parentName:"tr",align:"left"},"no")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("inlineCode",{parentName:"td"},"serializationSchema")),(0,r.kt)("td",{parentName:"tr",align:"left"},"SerializationSchema","[T]"),(0,r.kt)("td",{parentName:"tr",align:"left"},"serializer written"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SimpleStringSchema"),(0,r.kt)("td",{parentName:"tr",align:"left"},"no")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("inlineCode",{parentName:"td"},"partitioner")),(0,r.kt)("td",{parentName:"tr",align:"left"},"FlinkKafkaPartitioner","[T]"),(0,r.kt)("td",{parentName:"tr",align:"left"},"kafka partitioner"),(0,r.kt)("td",{parentName:"tr",align:"left"},"KafkaEqualityPartitioner","[T]"),(0,r.kt)("td",{parentName:"tr",align:"left"},"no")))),(0,r.kt)("h3",{id:"fault-tolerance-and-semantics"},"Fault Tolerance and Semantics"),(0,r.kt)("p",null,"After enabling Flink's ",(0,r.kt)("inlineCode",{parentName:"p"},"checkpointing"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"KafkaSink")," can provide ",(0,r.kt)("inlineCode",{parentName:"p"},"once-exactly")," semantic, please refer to Chapter 2 on ",(0,r.kt)("a",{parentName:"p",href:"/docs/development/conf/#checkpoints"},"project configuration")," for the specific setting of ",(0,r.kt)("inlineCode",{parentName:"p"},"checkpointing"),"."),(0,r.kt)("p",null,"In addition to enabling checkpointing for Flink, you can also choose from three different modes  by passing the appropriate ",(0,r.kt)("inlineCode",{parentName:"p"},"semantic")," parameters to ",(0,r.kt)("inlineCode",{parentName:"p"},"KafkaSink")),(0,r.kt)("div",{class:"counter"},(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"EXACTLY_ONCE  Provide exactly-once semantics with Kafka transactions"),(0,r.kt)("li",{parentName:"ul"},"AT_LEAST_ONCE At least once, it is guaranteed that no records will be lost (but records may be duplicated)"),(0,r.kt)("li",{parentName:"ul"},"NONE Flink There is no guarantee of semantics, and the resulting records may be lost or duplicated"))),(0,r.kt)("p",null,"configure ",(0,r.kt)("inlineCode",{parentName:"p"},"semantic")," under the ",(0,r.kt)("inlineCode",{parentName:"p"},"kafka.sink")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"kafka.sink:\n    bootstrap.servers: kfk1:9092,kfk2:9092,kfk3:9092\n    topic: kfk_sink\n    transaction.timeout.ms: 1000\n    semantic: AT_LEAST_ONCE # EXACTLY_ONCE|AT_LEAST_ONCE|NONE\n    batch.size: 1\n")),(0,r.kt)("details",null,(0,r.kt)("summary",null," kafka ",(0,r.kt)("code",null,"EXACTLY_ONCE")," Semantic Description"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"Semantic.EXACTLY_ONCE")," relies on the ability for transactions to be committed. Transaction commits occur before the checkpoint is triggered and after recovery from the checkpoint. If the time between a Flink application crash and a full restart exceeds Kafka's transaction timeout, then there will be caused data loss (Kafka automatically discards transactions that exceed the timeout). With this in mind, please configure the transaction timeout time based on the expected downtime."),(0,r.kt)("p",null,"By default, the Kafka broker sets transaction.max.timeout.ms to 15 minutes. This property does not allow setting the transaction timeout larger than producers value. By default, FlinkKafkaProducer sets the transaction.timeout.ms property in the producer config to 1 hour, so you should increase the transaction.max.timeout. ms before using Semantic."),(0,r.kt)("p",null,"In the read_committed mode of KafkaConsumer, any uncompleted (neither aborted nor completed) transaction will block all reads after the uncompleted transaction from the given Kafka topic. In other words, after following the following sequence of events."),(0,r.kt)("div",{class:"counter"},(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"User started transaction1 and used it to write some records"),(0,r.kt)("li",{parentName:"ul"},"User started transaction2 and used it to write some other records"),(0,r.kt)("li",{parentName:"ul"},"User committed transaction2"))),(0,r.kt)("p",null,"Even if the records in transaction2 are committed, they will not be visible to the consumer until transaction1 is committed or aborted. This has 2 levels of implications."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"First, during normal operation of the Flink application, the user can expect a delay in the visibility of records, it equals to the average time between completed checkpoints."),(0,r.kt)("li",{parentName:"ul"},"Second, in the case of a failed Flink application, the topic written by this application will be blocked until the application is restarted or the configured transaction timeout has elapsed, and normalcy will resume. This annotation only applies multi agents or applications writing to the same Kafka topic.")),(0,r.kt)("p",null,"Note: The ",(0,r.kt)("inlineCode",{parentName:"p"},"Semantic.EXACTLY_ONCE")," mode uses a fixed size pool of KafkaProducers for each FlinkKafkaProducer instance. Each checkpoint uses one of the producers. If the number of concurrent checkpoints exceeds the pool size, FlinkKafkaProducer will throw an exception and cause the entire application to fail. Please configure the maximum pool size and the maximum number of concurrent checkpoints wisely."),(0,r.kt)("p",null,"Note: ",(0,r.kt)("inlineCode",{parentName:"p"},"Semantic.EXACTLY_ONCE")," will do everything possible to not leave any stay transactions that would otherwise block other consumers from reading data from this Kafka topic. However, if a Flink application fails before the first checkpoint, there will be no information about the previous pool size in the system after restarting such an application. Therefore, it is not safe to scale down a Flink application before the first checkpoint is completed and the concurrent number of scaling is greater than the value of the safety factor FlinkKafkaProducer.SAFE_SCALE_DOWN_FACTOR.")),(0,r.kt)("h3",{id:"multiple-instance-kafka-specifies-alias"},"Multiple instance kafka specifies alias"),(0,r.kt)("p",null,"If there are multiple instances of kafka that need to be configured at the time of writing, used ",(0,r.kt)("inlineCode",{parentName:"p"},"alias")," to distinguish between multi kafka instances, configured as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"kafka.sink:\n    kafka_cluster1:\n        bootstrap.servers: kfk1:9092,kfk2:9092,kfk3:9092\n        topic: kfk_sink\n        transaction.timeout.ms: 1000\n        semantic: AT_LEAST_ONCE # EXACTLY_ONCE|AT_LEAST_ONCE|NONE\n        batch.size: 1\n    kafka_cluster2:\n        bootstrap.servers: kfk6:9092,kfk7:9092,kfk8:9092\n        topic: kfk_sink\n        transaction.timeout.ms: 1000\n        semantic: AT_LEAST_ONCE # EXACTLY_ONCE|AT_LEAST_ONCE|NONE\n        batch.size: 1\n")),(0,r.kt)("p",null,"When writing data, you need to manually specify ",(0,r.kt)("inlineCode",{parentName:"p"},"alias"),". Note that the ",(0,r.kt)("inlineCode",{parentName:"p"},"scala")," api and ",(0,r.kt)("inlineCode",{parentName:"p"},"java")," api are different in code. ",(0,r.kt)("inlineCode",{parentName:"p"},"scala")," specifies parameters directly in the ",(0,r.kt)("inlineCode",{parentName:"p"},"sink")," method, while the ",(0,r.kt)("inlineCode",{parentName:"p"},"java")," api is It is set by the ",(0,r.kt)("inlineCode",{parentName:"p"},"alias()")," method"),(0,r.kt)(i.Z,{mdxType:"Tabs"},(0,r.kt)(o.Z,{value:"Scala",label:"Scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},' val source = KafkaSource().getDataStream[String]().map(_.value)\n KafkaSink().sink(source,alias = "kafka_cluster1")\n'))),(0,r.kt)(o.Z,{value:"Java",label:"Java",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},' StreamEnvConfig envConfig = new StreamEnvConfig(args, null);\n StreamingContext context = new StreamingContext(envConfig);\n DataStream<String> source = new KafkaSource<String>(context)\n         .getDataStream()\n         .map((MapFunction<KafkaRecord<String>, String>) KafkaRecord::value);\n\n new KafkaSink<String>(context).alias("kafka_cluster1").sink(source);\n\n context.start();\n')))),(0,r.kt)("h3",{id:"specific-serializationschema"},"Specific SerializationSchema"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"Flink Kafka Producer")," needs know how to convert Java/Scala objects to binary data. KafkaSerializationSchema allows users to specify such a schema, please refer to the ",(0,r.kt)("a",{parentName:"p",href:"https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/kafka.html#the-serializationschema"},"official documentation")," for how to do this and documentation"),(0,r.kt)("p",null,"In ",(0,r.kt)("inlineCode",{parentName:"p"},"KafkaSink"),", the default serialization is not specified, and the ",(0,r.kt)("inlineCode",{parentName:"p"},"SimpleStringSchema")," is used for serialization, where the developer can specify a custom serializer, specified by the ",(0,r.kt)("inlineCode",{parentName:"p"},"serializationSchema")," parameter, for example, to write the ",(0,r.kt)("inlineCode",{parentName:"p"},"user")," object to a custom format ",(0,r.kt)("inlineCode",{parentName:"p"},"kafka")),(0,r.kt)(i.Z,{mdxType:"Tabs"},(0,r.kt)(o.Z,{value:"scala",label:"Scala",default:!0,mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import org.apache.streampark.common.util.JsonUtils\nimport org.apache.streampark.flink.core.scala.FlinkStreaming\nimport org.apache.flink.api.common.serialization.SerializationSchema\nimport org.apache.streampark.flink.core.scala.sink.JdbcSink\nimport org.apache.streampark.flink.core.scala.source.KafkaSource\nimport org.apache.flink.api.scala._\n\nobject KafkaSinkApp extends FlinkStreaming {\n\noverride def handle(): Unit = {\nval source = KafkaSource()\n.getDataStream[String]()\n.map(x => JsonUtils.read[User](x.value))\n\n    KafkaSink().sink[User](source, serialization = new SerializationSchema[User]() {\n      override def serialize(user: User): Array[Byte] = {\n        s"${user.name},${user.age},${user.gender},${user.address}".getBytes\n      }\n    })\n\n}\n\n}\n\ncase class User(name: String, age: Int, gender: Int, address: String)\n'))),(0,r.kt)(o.Z,{value:"Java",label:"Java",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},'import com.fasterxml.jackson.databind.ObjectMapper;\nimport org.apache.streampark.flink.core.java.function.StreamEnvConfigFunction;\nimport org.apache.streampark.flink.core.java.sink.KafkaSink;\nimport org.apache.streampark.flink.core.java.source.KafkaSource;\nimport org.apache.streampark.flink.core.scala.StreamingContext;\nimport org.apache.streampark.flink.core.scala.source.KafkaRecord;\nimport org.apache.streampark.flink.core.scala.util.StreamEnvConfig;\nimport org.apache.flink.api.common.functions.FilterFunction;\nimport org.apache.flink.api.common.functions.MapFunction;\nimport org.apache.flink.api.common.serialization.SerializationSchema;\nimport org.apache.flink.streaming.api.datastream.DataStream;\n\nimport java.io.Serializable;\n\npublic class kafkaSinkJavaApp {\n\n    public static void main(String[] args) {\n\n        StreamEnvConfig envConfig = new StreamEnvConfig(args, null);\n        StreamingContext context = new StreamingContext(envConfig);\n        ObjectMapper mapper = new ObjectMapper();\n\n        DataStream<JavaUser> source = new KafkaSource<String>(context)\n                .getDataStream()\n                .map((MapFunction<KafkaRecord<String>, JavaUser>) value ->\n                        mapper.readValue(value.value(), JavaUser.class));\n\n        new KafkaSink<JavaUser>(context)\n                .serializer(\n                        (SerializationSchema<JavaUser>) element ->\n                                String.format("%s,%d,%d,%s", element.name, element.age, element.gender, element.address).getBytes()\n                ).sink(source);\n\n        context.start();\n    }\n\n}\n\nclass JavaUser implements Serializable {\n    String name;\n    Integer age;\n    Integer gender;\n    String address;\n}\n')))),(0,r.kt)("h3",{id:"specific-serializationschema-1"},"Specific SerializationSchema"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"Flink Kafka Producer")," needs know how to convert Java/Scala objects to binary data. KafkaSerializationSchema allows users to specify such a schema, please refer to the ",(0,r.kt)("a",{parentName:"p",href:"https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/kafka.html#the-serializationschema"},"official documentation")," for how to do this and documentation"),(0,r.kt)("p",null,"In ",(0,r.kt)("inlineCode",{parentName:"p"},"KafkaSink"),", the default serialization is not specified, and the ",(0,r.kt)("inlineCode",{parentName:"p"},"SimpleStringSchema")," is used for serialization, where the developer can specify a custom serializer, specified by the ",(0,r.kt)("inlineCode",{parentName:"p"},"serializationSchema")," parameter, for example, to write the ",(0,r.kt)("inlineCode",{parentName:"p"},"user")," object to a custom format ",(0,r.kt)("inlineCode",{parentName:"p"},"kafka")),(0,r.kt)(i.Z,{mdxType:"Tabs"},(0,r.kt)(o.Z,{value:"scala",label:"Scala",default:!0,mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import org.apache.streampark.common.util.JsonUtils\nimport org.apache.streampark.flink.core.scala.FlinkStreaming\nimport org.apache.flink.api.common.serialization.SerializationSchema\nimport org.apache.streampark.flink.core.scala.sink.JdbcSink\nimport org.apache.streampark.flink.core.scala.source.KafkaSource\nimport org.apache.flink.api.scala._\n\nobject KafkaSinkApp extends FlinkStreaming {\n\noverride def handle(): Unit = {\nval source = KafkaSource()\n.getDataStream[String]()\n.map(x => JsonUtils.read[User](x.value))\n\n    KafkaSink().sink[User](source, serialization = new SerializationSchema[User]() {\n      override def serialize(user: User): Array[Byte] = {\n        s"${user.name},${user.age},${user.gender},${user.address}".getBytes\n      }\n    })\n\n}\n\n}\n\ncase class User(name: String, age: Int, gender: Int, address: String)\n'))),(0,r.kt)(o.Z,{value:"Java",label:"Java",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},'import com.fasterxml.jackson.databind.ObjectMapper;\nimport org.apache.streampark.flink.core.java.function.StreamEnvConfigFunction;\nimport org.apache.streampark.flink.core.java.sink.KafkaSink;\nimport org.apache.streampark.flink.core.java.source.KafkaSource;\nimport org.apache.streampark.flink.core.scala.StreamingContext;\nimport org.apache.streampark.flink.core.scala.source.KafkaRecord;\nimport org.apache.streampark.flink.core.scala.util.StreamEnvConfig;\nimport org.apache.flink.api.common.functions.FilterFunction;\nimport org.apache.flink.api.common.functions.MapFunction;\nimport org.apache.flink.api.common.serialization.SerializationSchema;\nimport org.apache.flink.streaming.api.datastream.DataStream;\n\nimport java.io.Serializable;\n\npublic class kafkaSinkJavaApp {\n\n    public static void main(String[] args) {\n\n        StreamEnvConfig envConfig = new StreamEnvConfig(args, null);\n        StreamingContext context = new StreamingContext(envConfig);\n        ObjectMapper mapper = new ObjectMapper();\n\n        DataStream<JavaUser> source = new KafkaSource<String>(context)\n                .getDataStream()\n                .map((MapFunction<KafkaRecord<String>, JavaUser>) value ->\n                        mapper.readValue(value.value(), JavaUser.class));\n\n        new KafkaSink<JavaUser>(context)\n                .serializer(\n                        (SerializationSchema<JavaUser>) element ->\n                                String.format("%s,%d,%d,%s", element.name, element.age, element.gender, element.address).getBytes()\n                ).sink(source);\n\n        context.start();\n    }\n\n}\n\nclass JavaUser implements Serializable {\n    String name;\n    Integer age;\n    Integer gender;\n    String address;\n}\n')))),(0,r.kt)("h3",{id:"specific-partitioner"},"specific partitioner"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"KafkaSink")," allows you to specify a kafka partitioner, if you don't specify it, the default is to use ",(0,r.kt)("inlineCode",{parentName:"p"},"StreamPark")," built-in ",(0,r.kt)("strong",{parentName:"p"},"KafkaEqualityPartitioner")," partitioner, as the name, the partitioner can write data to each partition evenly, the ",(0,r.kt)("inlineCode",{parentName:"p"},"scala")," api is set by the ",(0,r.kt)("inlineCode",{parentName:"p"}," partitioner")," parameter to set the partitioner,\n",(0,r.kt)("inlineCode",{parentName:"p"},"java")," api is set by ",(0,r.kt)("inlineCode",{parentName:"p"},"partitioner()")," method"),(0,r.kt)("admonition",{title:"Cautions",type:"tip"},(0,r.kt)("p",{parentName:"admonition"},"The default partitioner used in Flink Kafka Connector is ",(0,r.kt)("strong",{parentName:"p"},"FlinkFixedPartitioner"),", which requires special attention to the parallelism of ",(0,r.kt)("inlineCode",{parentName:"p"},"sink")," and the number of partitions of ",(0,r.kt)("inlineCode",{parentName:"p"},"kafka"),", otherwise it will write to a partition")))}d.isMDXComponent=!0},7198:(e,a,t)=>{t.d(a,{Z:()=>n});const n=t.p+"assets/images/streampark_kafkaapi-c3eb75726e1ec7cb8788dea846c54933.jpeg"}}]);